"""
Symbolic DAG Cache Management

Provides content-addressed caching for expensive symbolic elimination operations.
The symbolic elimination is O(nÂ³) and runs before JAX compilation. By caching
the result based on graph structure, we achieve 10-1000x speedups for repeated
model evaluations with different parameters.

Key Features:
- Content-addressed storage using graph hash
- Local cache with SQLite index
- Optional shared/distributed cache support
- Automatic cache validation and cleanup
- Import/export for model libraries

Example Usage:
    >>> from ptdalgorithms import Graph
    >>> from ptdalgorithms.symbolic_cache import SymbolicCache
    >>>
    >>> # Build parameterized graph
    >>> g = Graph(callback=my_callback, parameterized=True)
    >>>
    >>> # Get or compute symbolic DAG (cached automatically)
    >>> cache = SymbolicCache()
    >>> symbolic_dag = cache.get_or_compute(g)
    >>>
    >>> # Subsequent calls with same structure are instant
    >>> symbolic_dag2 = cache.get_or_compute(g)  # From cache!
"""

import os
import json
import sqlite3
import hashlib
from pathlib import Path
from typing import Optional, Dict, Any, List, Tuple
from datetime import datetime
import tempfile
import shutil

try:
    from . import ptdalgorithmscpp_pybind as cpp
    HAS_CPP = True
except ImportError:
    HAS_CPP = False
    cpp = None


class SymbolicCache:
    """
    Content-addressed cache for symbolic DAGs.

    The cache stores pre-computed symbolic elimination results, enabling
    fast instantiation for graphs with the same structure but different
    parameter values.

    Parameters
    ----------
    cache_dir : Path or str, optional
        Directory for cache storage. Default: ~/.ptdalgorithms_cache/symbolic
    shared_cache_dir : Path or str, optional
        Optional shared cache directory (read-only, checked after local)
    max_cache_size_gb : float, optional
        Maximum cache size in GB. Old entries are evicted when exceeded.
        Default: 10.0 GB
    enable_stats : bool, optional
        Enable cache hit/miss statistics tracking. Default: True

    Attributes
    ----------
    hits : int
        Number of cache hits (if enable_stats=True)
    misses : int
        Number of cache misses (if enable_stats=True)
    """

    def __init__(
        self,
        cache_dir: Optional[Path] = None,
        shared_cache_dir: Optional[Path] = None,
        max_cache_size_gb: float = 10.0,
        enable_stats: bool = True
    ):
        self.cache_dir = Path(cache_dir) if cache_dir else (
            Path.home() / '.ptdalgorithms_cache' / 'symbolic'
        )
        self.shared_cache_dir = Path(shared_cache_dir) if shared_cache_dir else None
        self.max_cache_size_bytes = int(max_cache_size_gb * 1024**3)
        self.enable_stats = enable_stats

        # Statistics
        self.hits = 0
        self.misses = 0

        # Create cache directory
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # Initialize SQLite index
        self.db_path = self.cache_dir / 'index.db'
        self._init_database()

    def _init_database(self):
        """Initialize SQLite database for cache index"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute('''
                CREATE TABLE IF NOT EXISTS cache_entries (
                    hash_key TEXT PRIMARY KEY,
                    file_path TEXT NOT NULL,
                    created_at TEXT NOT NULL,
                    accessed_at TEXT NOT NULL,
                    size_bytes INTEGER NOT NULL,
                    vertices INTEGER,
                    edges INTEGER,
                    elimination_time_ms REAL,
                    ptdalgorithms_version TEXT,
                    metadata TEXT
                )
            ''')
            conn.execute('''
                CREATE INDEX IF NOT EXISTS idx_accessed
                ON cache_entries(accessed_at)
            ''')
            conn.commit()

    def _compute_graph_hash(self, graph) -> str:
        """
        Compute content hash of graph structure.

        For now, uses Python-side hashing of serialized structure.
        TODO: Use C-level ptd_graph_content_hash() once bindings are ready.
        """
        import numpy as np

        # Serialize graph structure
        serialized = graph.serialize()

        # Create hash from structure (not parameter values!)
        hash_dict = {
            'state_length': int(serialized.get('state_length', 0)),
            'param_length': int(serialized.get('param_length', 0)),
            'n_vertices': int(serialized.get('n_vertices', 0)),
        }

        # Convert numpy arrays to lists for JSON serialization
        # Hash topology: regular edges
        if 'edges' in serialized:
            edges = serialized['edges']
            if isinstance(edges, np.ndarray):
                hash_dict['edges'] = edges.tolist()
            else:
                hash_dict['edges'] = edges

        # Hash start edges
        if 'start_edges' in serialized:
            start_edges = serialized['start_edges']
            if isinstance(start_edges, np.ndarray):
                hash_dict['start_edges'] = start_edges.tolist()
            else:
                hash_dict['start_edges'] = start_edges

        # Hash parameterized edges
        if 'param_edges' in serialized:
            param_edges = serialized['param_edges']
            if isinstance(param_edges, np.ndarray):
                hash_dict['param_edges'] = param_edges.tolist()
            else:
                hash_dict['param_edges'] = param_edges

        # Hash start parameterized edges
        if 'start_param_edges' in serialized:
            start_param_edges = serialized['start_param_edges']
            if isinstance(start_param_edges, np.ndarray):
                hash_dict['start_param_edges'] = start_param_edges.tolist()
            else:
                hash_dict['start_param_edges'] = start_param_edges

        # Hash vertex states
        if 'states' in serialized:
            states = serialized['states']
            if isinstance(states, np.ndarray):
                hash_dict['states'] = states.tolist()
            else:
                hash_dict['states'] = states

        # Create deterministic JSON and hash it
        json_str = json.dumps(hash_dict, sort_keys=True)
        hash_obj = hashlib.sha256(json_str.encode('utf-8'))
        return hash_obj.hexdigest()

    def exists(self, hash_key: str) -> bool:
        """Check if symbolic DAG exists in cache"""
        # Check local cache
        local_path = self.cache_dir / f"{hash_key}.json"
        if local_path.exists():
            return True

        # Check shared cache if configured
        if self.shared_cache_dir:
            shared_path = self.shared_cache_dir / f"{hash_key}.json"
            if shared_path.exists():
                return True

        return False

    def load(self, hash_key: str) -> Optional[str]:
        """
        Load symbolic DAG JSON from cache.

        Returns
        -------
        str or None
            JSON string of symbolic DAG, or None if not found
        """
        # Try local cache first
        local_path = self.cache_dir / f"{hash_key}.json"
        if local_path.exists():
            with open(local_path, 'r') as f:
                symbolic_json = f.read()

            # Update access time in database
            with sqlite3.connect(self.db_path) as conn:
                conn.execute(
                    'UPDATE cache_entries SET accessed_at = ? WHERE hash_key = ?',
                    (datetime.now().isoformat(), hash_key)
                )
                conn.commit()

            if self.enable_stats:
                self.hits += 1

            return symbolic_json

        # Try shared cache
        if self.shared_cache_dir:
            shared_path = self.shared_cache_dir / f"{hash_key}.json"
            if shared_path.exists():
                with open(shared_path, 'r') as f:
                    symbolic_json = f.read()

                # Copy to local cache for faster future access
                self._copy_to_local(hash_key, shared_path)

                if self.enable_stats:
                    self.hits += 1

                return symbolic_json

        if self.enable_stats:
            self.misses += 1

        return None

    def save(self, hash_key: str, symbolic_json: str, metadata: Optional[Dict[str, Any]] = None):
        """
        Save symbolic DAG to cache.

        Parameters
        ----------
        hash_key : str
            Content hash of the graph
        symbolic_json : str
            Serialized symbolic DAG (from ptd_graph_symbolic_to_json)
        metadata : dict, optional
            Additional metadata (vertices, edges, timing, etc.)
        """
        file_path = self.cache_dir / f"{hash_key}.json"

        # Write symbolic DAG
        with open(file_path, 'w') as f:
            f.write(symbolic_json)

        # Get file size
        size_bytes = file_path.stat().st_size

        # Store in database index
        now = datetime.now().isoformat()
        metadata = metadata or {}

        # Get package version
        try:
            from . import __version__
            version = __version__
        except:
            version = "unknown"

        with sqlite3.connect(self.db_path) as conn:
            conn.execute('''
                INSERT OR REPLACE INTO cache_entries
                (hash_key, file_path, created_at, accessed_at, size_bytes,
                 vertices, edges, elimination_time_ms, ptdalgorithms_version, metadata)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                hash_key,
                str(file_path),
                now,
                now,
                size_bytes,
                metadata.get('vertices'),
                metadata.get('edges'),
                metadata.get('elimination_time_ms'),
                version,
                json.dumps(metadata)
            ))
            conn.commit()

        # Check cache size and evict if needed
        self._check_cache_size()

    def _copy_to_local(self, hash_key: str, source_path: Path):
        """Copy entry from shared cache to local cache"""
        dest_path = self.cache_dir / f"{hash_key}.json"
        shutil.copy2(source_path, dest_path)

        # Add to local database
        size_bytes = dest_path.stat().st_size
        now = datetime.now().isoformat()

        with sqlite3.connect(self.db_path) as conn:
            conn.execute('''
                INSERT OR IGNORE INTO cache_entries
                (hash_key, file_path, created_at, accessed_at, size_bytes,
                 vertices, edges, elimination_time_ms, ptdalgorithms_version, metadata)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                hash_key, str(dest_path), now, now, size_bytes,
                None, None, None, "shared", "{}"
            ))
            conn.commit()

    def _check_cache_size(self):
        """Check total cache size and evict old entries if needed"""
        with sqlite3.connect(self.db_path) as conn:
            # Get total size
            cursor = conn.execute('SELECT SUM(size_bytes) FROM cache_entries')
            total_size = cursor.fetchone()[0] or 0

            if total_size > self.max_cache_size_bytes:
                # Evict least recently accessed entries
                bytes_to_free = total_size - self.max_cache_size_bytes

                cursor = conn.execute('''
                    SELECT hash_key, file_path, size_bytes
                    FROM cache_entries
                    ORDER BY accessed_at ASC
                ''')

                freed = 0
                for hash_key, file_path, size_bytes in cursor:
                    if freed >= bytes_to_free:
                        break

                    # Delete file
                    Path(file_path).unlink(missing_ok=True)

                    # Delete metadata file
                    meta_path = Path(file_path).with_suffix('.meta')
                    meta_path.unlink(missing_ok=True)

                    # Remove from database
                    conn.execute('DELETE FROM cache_entries WHERE hash_key = ?', (hash_key,))

                    freed += size_bytes

                conn.commit()

    def get_or_compute(self, graph) -> str:
        """
        Get cached symbolic DAG or compute it.

        This is the main entry point for cache usage.

        Parameters
        ----------
        graph : Graph
            Parameterized graph to get symbolic DAG for

        Returns
        -------
        str
            JSON string of symbolic DAG (ready for ptd_graph_symbolic_from_json)
        """
        # Compute content hash
        hash_key = self._compute_graph_hash(graph)

        # Try to load from cache
        symbolic_json = self.load(hash_key)
        if symbolic_json is not None:
            return symbolic_json

        # Not in cache - compute it (expensive O(nÂ³) operation!)
        import time
        start_time = time.time()

        # TODO: Call C function ptd_graph_symbolic_elimination
        # For now, return placeholder that signals "not implemented"
        # This will be integrated when C++ bindings are ready

        # Placeholder: In actual implementation, this would call:
        # symbolic_graph = cpp.graph_symbolic_elimination(graph.c_graph())
        # symbolic_json = cpp.graph_symbolic_to_json(symbolic_graph)

        # For now, just cache the serialized graph itself as a marker
        # Convert numpy arrays to lists for JSON serialization
        import numpy as np
        serialized = graph.serialize()
        serialized_json_safe = {}
        for key, value in serialized.items():
            if isinstance(value, np.ndarray):
                serialized_json_safe[key] = value.tolist()
            else:
                serialized_json_safe[key] = value

        symbolic_json = json.dumps({
            'placeholder': True,
            'message': 'C++ symbolic elimination not yet integrated',
            'graph_hash': hash_key,
            'serialized_graph': serialized_json_safe
        })

        elimination_time_ms = (time.time() - start_time) * 1000

        # Save to cache
        metadata = {
            'vertices': graph.vertices_length(),
            'edges': sum(len(v.edges()) for v in graph.vertices()),
            'elimination_time_ms': elimination_time_ms
        }
        self.save(hash_key, symbolic_json, metadata)

        return symbolic_json

    def clear(self):
        """Clear all cache entries"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute('SELECT file_path FROM cache_entries')
            for (file_path,) in cursor:
                Path(file_path).unlink(missing_ok=True)
                Path(file_path).with_suffix('.meta').unlink(missing_ok=True)

            conn.execute('DELETE FROM cache_entries')
            conn.commit()

    def info(self) -> Dict[str, Any]:
        """Get cache statistics"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute('''
                SELECT
                    COUNT(*) as num_entries,
                    SUM(size_bytes) as total_bytes,
                    AVG(vertices) as avg_vertices,
                    AVG(elimination_time_ms) as avg_elimination_ms
                FROM cache_entries
            ''')
            row = cursor.fetchone()

            return {
                'cache_dir': str(self.cache_dir),
                'num_entries': row[0] or 0,
                'total_size_mb': (row[1] or 0) / (1024**2),
                'avg_vertices': row[2] or 0,
                'avg_elimination_time_ms': row[3] or 0,
                'hits': self.hits if self.enable_stats else None,
                'misses': self.misses if self.enable_stats else None,
                'hit_rate': (
                    self.hits / (self.hits + self.misses)
                    if self.enable_stats and (self.hits + self.misses) > 0
                    else None
                )
            }

    def list_entries(self, limit: int = 100) -> List[Dict[str, Any]]:
        """List cache entries with metadata"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute('''
                SELECT hash_key, created_at, accessed_at, size_bytes,
                       vertices, edges, elimination_time_ms, ptdalgorithms_version
                FROM cache_entries
                ORDER BY accessed_at DESC
                LIMIT ?
            ''', (limit,))

            entries = []
            for row in cursor:
                entries.append({
                    'hash_key': row[0],
                    'created_at': row[1],
                    'accessed_at': row[2],
                    'size_kb': row[3] / 1024,
                    'vertices': row[4],
                    'edges': row[5],
                    'elimination_time_ms': row[6],
                    'version': row[7]
                })

            return entries

    def export_library(self, output_dir: Path, hash_keys: Optional[List[str]] = None):
        """
        Export cache entries to a model library directory.

        Creates a distributable directory structure with cached symbolic DAGs.

        Parameters
        ----------
        output_dir : Path
            Directory to export to
        hash_keys : list of str, optional
            Specific hash keys to export. If None, exports all.
        """
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        with sqlite3.connect(self.db_path) as conn:
            if hash_keys:
                placeholders = ','.join('?' * len(hash_keys))
                cursor = conn.execute(
                    f'SELECT hash_key, file_path, metadata FROM cache_entries WHERE hash_key IN ({placeholders})',
                    hash_keys
                )
            else:
                cursor = conn.execute('SELECT hash_key, file_path, metadata FROM cache_entries')

            exported = []
            for hash_key, file_path, metadata_json in cursor:
                # Copy symbolic DAG
                src = Path(file_path)
                dst = output_dir / f"{hash_key}.json"
                shutil.copy2(src, dst)

                # Copy metadata
                metadata = json.loads(metadata_json) if metadata_json else {}
                meta_dst = output_dir / f"{hash_key}.meta"
                with open(meta_dst, 'w') as f:
                    json.dump(metadata, f, indent=2)

                exported.append(hash_key)

            # Create manifest
            manifest = {
                'exported_at': datetime.now().isoformat(),
                'num_entries': len(exported),
                'hash_keys': exported
            }
            with open(output_dir / 'manifest.json', 'w') as f:
                json.dump(manifest, f, indent=2)

        print(f"â Exported {len(exported)} cache entries to {output_dir}")

    def import_library(self, library_dir: Path):
        """
        Import cache entries from a model library directory.

        Parameters
        ----------
        library_dir : Path
            Directory containing exported cache entries
        """
        library_dir = Path(library_dir)

        # Read manifest
        manifest_path = library_dir / 'manifest.json'
        if manifest_path.exists():
            with open(manifest_path, 'r') as f:
                manifest = json.load(f)
            hash_keys = manifest.get('hash_keys', [])
        else:
            # No manifest, scan directory
            hash_keys = [
                p.stem for p in library_dir.glob('*.json')
                if p.name != 'manifest.json'
            ]

        imported = 0
        for hash_key in hash_keys:
            src = library_dir / f"{hash_key}.json"
            if not src.exists():
                continue

            # Read symbolic DAG
            with open(src, 'r') as f:
                symbolic_json = f.read()

            # Read metadata if available
            meta_src = library_dir / f"{hash_key}.meta"
            metadata = {}
            if meta_src.exists():
                with open(meta_src, 'r') as f:
                    metadata = json.load(f)

            # Save to cache
            self.save(hash_key, symbolic_json, metadata)
            imported += 1

        print(f"â Imported {imported} cache entries from {library_dir}")


def print_cache_info(cache: Optional[SymbolicCache] = None):
    """Print formatted cache information"""
    if cache is None:
        cache = SymbolicCache()

    info = cache.info()

    print("=" * 70)
    print("SYMBOLIC DAG CACHE INFO")
    print("=" * 70)
    print(f"Cache directory: {info['cache_dir']}")
    print(f"Cached DAGs: {info['num_entries']}")
    print(f"Total size: {info['total_size_mb']:.1f} MB")

    if info['num_entries'] > 0:
        print(f"Average vertices: {info['avg_vertices']:.0f}")
        print(f"Average elimination time: {info['avg_elimination_time_ms']:.1f} ms")

    if info['hits'] is not None:
        print(f"\nStatistics:")
        print(f"  Cache hits: {info['hits']}")
        print(f"  Cache misses: {info['misses']}")
        if info['hit_rate'] is not None:
            print(f"  Hit rate: {info['hit_rate']*100:.1f}%")

    print("=" * 70)
