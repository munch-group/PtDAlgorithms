{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Distributed Computing Basics\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Computing Basics\n",
    "\n",
    "This guide introduces the distributed computing capabilities in PtDAlgorithms, showing how to scale computations from a single machine to 100+ node clusters with minimal code changes.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The new distributed computing interface provides:\n",
    "\n",
    "- **One-line initialization** - Replace 200+ lines of SLURM boilerplate\n",
    "- **Automatic environment detection** - Works locally and on SLURM clusters\n",
    "- **JAX integration** - Full support for pmap/vmap parallelization\n",
    "- **Configuration management** - YAML-based cluster configs\n",
    "\n",
    "## The Problem: SLURM Boilerplate\n",
    "\n",
    "### Before (200+ lines)\n",
    "\n",
    "```python\n",
    "# Detect SLURM environment\n",
    "if 'SLURM_JOB_ID' in os.environ:\n",
    "    num_processes = int(os.environ['SLURM_NTASKS'])\n",
    "    process_id = int(os.environ['SLURM_PROCID'])\n",
    "    cpus_per_task = int(os.environ.get('SLURM_CPUS_PER_TASK', '1'))\n",
    "\n",
    "    # Get coordinator node\n",
    "    nodelist = os.environ['SLURM_JOB_NODELIST']\n",
    "    result = subprocess.run(['scontrol', 'show', 'hostnames', nodelist], ...)\n",
    "    coordinator_node = result.stdout.strip().split('\\n')[0]\n",
    "\n",
    "    # Setup environment\n",
    "    os.environ['XLA_FLAGS'] = f'--xla_force_host_platform_device_count={cpus_per_task}'\n",
    "\n",
    "    # Initialize JAX distributed\n",
    "    coordinator_address = f\"{coordinator_node}:{coordinator_port}\"\n",
    "    jax.distributed.initialize(...)\n",
    "\n",
    "# ... 150+ more lines ...\n",
    "```\n",
    "\n",
    "### After (1 line)\n",
    "\n",
    "```python\n",
    "from ptdalgorithms import initialize_distributed\n",
    "\n",
    "dist_info = initialize_distributed()\n",
    "```\n",
    "\n",
    "That's it! All SLURM detection, coordinator setup, and JAX initialization happens automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start Example\n",
    "\n",
    "Let's see how easy distributed computing can be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Not running under SLURM - using single-node setup\n",
      "[INFO] Configured JAX for 1 CPU devices\n",
      "[INFO] JAX x64 precision enabled\n",
      "[INFO] Single-node setup - no distributed initialization needed\n",
      "[INFO] \n",
      "Distributed Configuration:\n",
      "  Job ID: N/A\n",
      "  Process: 0/1\n",
      "  Coordinator: localhost:12345 (this node)\n",
      "  Local devices: 1\n",
      "  Global devices: 1\n",
      "  Platform: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process ID: 0\n",
      "Total processes: 1\n",
      "Local devices: 1\n",
      "Global devices: 1\n",
      "Is coordinator: True\n",
      "\n",
      "Full configuration:\n",
      "Distributed Configuration:\n",
      "  Job ID: N/A\n",
      "  Process: 0/1\n",
      "  Coordinator: localhost:12345 (this node)\n",
      "  Local devices: 1\n",
      "  Global devices: 1\n",
      "  Platform: cpu\n"
     ]
    }
   ],
   "source": [
    "from ptdalgorithms import initialize_distributed\n",
    "\n",
    "# Initialize distributed computing (handles everything automatically)\n",
    "dist_info = initialize_distributed(\n",
    "    coordinator_port=12345,\n",
    "    platform=\"cpu\",\n",
    "    enable_x64=True\n",
    ")\n",
    "\n",
    "# Check configuration\n",
    "print(f\"Process ID: {dist_info.process_id}\")\n",
    "print(f\"Total processes: {dist_info.num_processes}\")\n",
    "print(f\"Local devices: {dist_info.local_device_count}\")\n",
    "print(f\"Global devices: {dist_info.global_device_count}\")\n",
    "print(f\"Is coordinator: {dist_info.is_coordinator}\")\n",
    "print(f\"\\nFull configuration:\")\n",
    "print(dist_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistributedConfig Object\n",
    "\n",
    "The `initialize_distributed()` function returns a `DistributedConfig` object with all the information you need:\n",
    "\n",
    "| Attribute | Description |\n",
    "|-----------|-------------|\n",
    "| `num_processes` | Total number of processes (nodes) |\n",
    "| `process_id` | This process's rank (0 to num_processes-1) |\n",
    "| `local_device_count` | Number of devices on this node |\n",
    "| `global_device_count` | Total devices across all nodes |\n",
    "| `is_coordinator` | True if this is the coordinator (rank 0) |\n",
    "| `coordinator_address` | Address of coordinator (\"host:port\") |\n",
    "| `job_id` | SLURM job ID (if running under SLURM) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Distributed Computation\n",
    "\n",
    "Let's build a complete example that evaluates a phase-type distribution across multiple devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built Erlang(5) distribution with 6 states\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from ptdalgorithms import Graph\n",
    "\n",
    "def build_erlang_model(num_stages=5):\n",
    "    \"\"\"\n",
    "    Build an Erlang distribution (sum of exponentials).\n",
    "    \n",
    "    This represents the time until the num_stages'th event\n",
    "    in a Poisson process.\n",
    "    \"\"\"\n",
    "    g = Graph(1)\n",
    "    start = g.starting_vertex()\n",
    "    \n",
    "    # Create chain of states\n",
    "    vertices = [start]\n",
    "    for i in range(num_stages):\n",
    "        v = g.find_or_create_vertex([i + 1])\n",
    "        vertices.append(v)\n",
    "    \n",
    "    # Add edges with rate 1.0\n",
    "    for i in range(num_stages):\n",
    "        vertices[i].add_edge(vertices[i + 1], 1.0)\n",
    "    \n",
    "    return g\n",
    "\n",
    "# Build model\n",
    "graph = build_erlang_model(num_stages=5)\n",
    "print(f\"Built Erlang(5) distribution with {graph.vertices_length()} states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel PDF Evaluation\n",
    "\n",
    "Now let's evaluate the PDF at multiple time points in parallel using JAX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConcretizationTypeError",
     "evalue": "Abstract tracer value encountered where concrete value is expected: traced array with shape float64[]\nThe problem arose with the `float` function. If trying to convert the data type of a value, try using `x.astype(float)` or `jnp.array(x, float)` instead.\nThis BatchTracer with object id 5450445360 was created on line:\n  /var/folders/s6/srs8qkh52w1_h32d65z95tth0000gn/T/ipykernel_72389/2297681017.py:27:17 (evaluate_pdf_parallel)\n\nSee https://docs.jax.dev/en/latest/errors.html#jax.errors.ConcretizationTypeError",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConcretizationTypeError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m time_points = np.linspace(\u001b[32m0.1\u001b[39m, \u001b[32m10.0\u001b[39m, \u001b[32m32\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Evaluate in parallel\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m times, pdf_vals = \u001b[43mevaluate_pdf_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdist_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Display results\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dist_info.is_coordinator:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mevaluate_pdf_parallel\u001b[39m\u001b[34m(graph, time_points, dist_info)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m     26\u001b[39m time_points_jax = jnp.array(time_points_reshaped)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m pdf_values = \u001b[43mpmap_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_points_jax\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Flatten results\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m time_points_reshaped.flatten(), pdf_values.flatten()\n",
      "    \u001b[31m[... skipping hidden 21 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mevaluate_pdf_parallel.<locals>.eval_pdf\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34meval_pdf\u001b[39m(t):\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m graph.pdf(\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m)\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PtDAlgorithms/.pixi/envs/default/lib/python3.13/site-packages/jax/_src/core.py:1812\u001b[39m, in \u001b[36mconcretization_function_error.<locals>.error\u001b[39m\u001b[34m(self, arg)\u001b[39m\n\u001b[32m   1811\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34merror\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg):\n\u001b[32m-> \u001b[39m\u001b[32m1812\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ConcretizationTypeError(arg, fname_context)\n",
      "\u001b[31mConcretizationTypeError\u001b[39m: Abstract tracer value encountered where concrete value is expected: traced array with shape float64[]\nThe problem arose with the `float` function. If trying to convert the data type of a value, try using `x.astype(float)` or `jnp.array(x, float)` instead.\nThis BatchTracer with object id 5450445360 was created on line:\n  /var/folders/s6/srs8qkh52w1_h32d65z95tth0000gn/T/ipykernel_72389/2297681017.py:27:17 (evaluate_pdf_parallel)\n\nSee https://docs.jax.dev/en/latest/errors.html#jax.errors.ConcretizationTypeError"
     ]
    }
   ],
   "source": [
    "def evaluate_pdf_parallel(graph, time_points, dist_info):\n",
    "    \"\"\"\n",
    "    Evaluate PDF at multiple time points using parallel computation.\n",
    "    \"\"\"\n",
    "    # Determine how many points per device\n",
    "    n_points = len(time_points)\n",
    "    points_per_device = n_points // dist_info.local_device_count\n",
    "    \n",
    "    # Reshape for pmap: (n_devices, points_per_device)\n",
    "    time_points_reshaped = time_points[:dist_info.local_device_count * points_per_device]\n",
    "    time_points_reshaped = time_points_reshaped.reshape(\n",
    "        (dist_info.local_device_count, points_per_device)\n",
    "    )\n",
    "    \n",
    "    # Define PDF evaluation function\n",
    "    def eval_pdf(t):\n",
    "        return graph.pdf(float(t))\n",
    "    \n",
    "    # Vectorize over time points on each device\n",
    "    vmap_pdf = jax.vmap(eval_pdf)\n",
    "    \n",
    "    # Parallelize across devices\n",
    "    pmap_pdf = jax.pmap(vmap_pdf)\n",
    "    \n",
    "    # Evaluate\n",
    "    time_points_jax = jnp.array(time_points_reshaped)\n",
    "    pdf_values = pmap_pdf(time_points_jax)\n",
    "    \n",
    "    # Flatten results\n",
    "    return time_points_reshaped.flatten(), pdf_values.flatten()\n",
    "\n",
    "# Generate time points\n",
    "time_points = np.linspace(0.1, 10.0, 32)\n",
    "\n",
    "# Evaluate in parallel\n",
    "times, pdf_vals = evaluate_pdf_parallel(graph, time_points, dist_info)\n",
    "\n",
    "# Display results\n",
    "if dist_info.is_coordinator:\n",
    "    print(f\"\\nEvaluated {len(times)} time points in parallel\")\n",
    "    print(f\"Distributed across {dist_info.global_device_count} devices\")\n",
    "    print(f\"\\nSample values:\")\n",
    "    for i in range(min(5, len(times))):\n",
    "        print(f\"  t={times[i]:.2f} -> PDF={pdf_vals[i]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "Let's visualize the PDF we just computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if dist_info.is_coordinator:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(times, pdf_vals, 'b-', linewidth=2, label='Erlang(5) PDF')\n",
    "    plt.xlabel('Time', fontsize=12)\n",
    "    plt.ylabel('Probability Density', fontsize=12)\n",
    "    plt.title('Erlang Distribution PDF (Computed in Parallel)', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nPDF computed using {dist_info.global_device_count} parallel devices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### 1. Use Coordinator Check for Output\n",
    "\n",
    "Only the coordinator (rank 0) should print summary information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dist_info.is_coordinator:\n",
    "    print(f\"Starting computation with {dist_info.global_device_count} devices\")\n",
    "    # ... other logging ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Distribute Work Evenly\n",
    "\n",
    "Ensure work is divisible by device count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good: evenly divisible\n",
    "n_particles = dist_info.global_device_count * 4  # Exactly 4 per device\n",
    "\n",
    "# Bad: not evenly divisible\n",
    "# n_particles = 37  # Won't divide evenly\n",
    "\n",
    "print(f\"Using {n_particles} particles across {dist_info.global_device_count} devices\")\n",
    "print(f\"= {n_particles // dist_info.global_device_count} particles per device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Use Different Seeds per Process\n",
    "\n",
    "Avoid identical random numbers across processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set unique seed for each process\n",
    "np.random.seed(42 + dist_info.process_id)\n",
    "\n",
    "# Generate some random numbers (different on each process)\n",
    "random_vals = np.random.randn(5)\n",
    "print(f\"Process {dist_info.process_id} random values: {random_vals[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running on SLURM Clusters\n",
    "\n",
    "The same code works seamlessly on SLURM clusters! No code changes needed.\n",
    "\n",
    "### Local Testing\n",
    "\n",
    "```bash\n",
    "# Test on your laptop\n",
    "python my_script.py\n",
    "```\n",
    "\n",
    "### SLURM Submission\n",
    "\n",
    "```bash\n",
    "# Submit to cluster (no code changes!)\n",
    "sbatch <(python generate_slurm_script.py --profile medium --script my_script.py)\n",
    "```\n",
    "\n",
    "### What Happens Automatically\n",
    "\n",
    "When running on SLURM, `initialize_distributed()` automatically:\n",
    "\n",
    "1. Detects SLURM environment variables\n",
    "2. Identifies coordinator node\n",
    "3. Configures JAX devices\n",
    "4. Initializes distributed JAX\n",
    "5. Sets up inter-node communication\n",
    "\n",
    "When running locally, it:\n",
    "\n",
    "1. Creates multiple local devices\n",
    "2. Enables CPU parallelization\n",
    "3. Works just like SLURM mode (but single node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Scaling\n",
    "\n",
    "Let's demonstrate how computation scales with device count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_parallel_computation(graph, n_evaluations, dist_info):\n",
    "    \"\"\"\n",
    "    Benchmark parallel PDF evaluation.\n",
    "    \"\"\"\n",
    "    # Generate time points\n",
    "    time_points = np.linspace(0.1, 10.0, n_evaluations)\n",
    "    \n",
    "    # Warm up (JIT compilation)\n",
    "    _, _ = evaluate_pdf_parallel(graph, time_points[:16], dist_info)\n",
    "    \n",
    "    # Benchmark\n",
    "    start = time.time()\n",
    "    times, pdf_vals = evaluate_pdf_parallel(graph, time_points, dist_info)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    return elapsed, len(times)\n",
    "\n",
    "if dist_info.is_coordinator:\n",
    "    print(\"\\nPerformance Benchmark\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test different workload sizes\n",
    "    for n in [32, 64, 128]:\n",
    "        elapsed, n_computed = benchmark_parallel_computation(graph, n, dist_info)\n",
    "        throughput = n_computed / elapsed\n",
    "        \n",
    "        print(f\"\\nEvaluations: {n_computed}\")\n",
    "        print(f\"  Time: {elapsed:.4f}s\")\n",
    "        print(f\"  Throughput: {throughput:.1f} evals/sec\")\n",
    "        print(f\"  Devices: {dist_info.global_device_count}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you understand the basics, check out:\n",
    "\n",
    "1. **[Distributed SVGD Inference](distributed_svgd_inference.ipynb)** - Bayesian inference across multiple nodes\n",
    "2. **[SLURM Cluster Setup](slurm_cluster_setup.ipynb)** - Configure and manage cluster resources\n",
    "3. **[API Reference](../api/index.html)** - Complete API documentation\n",
    "\n",
    "## Summary\n",
    "\n",
    "**One-line initialization** replaces 200+ lines of boilerplate\n",
    "\n",
    "**Automatic environment detection** works locally and on SLURM\n",
    "\n",
    "**Full JAX integration** with pmap/vmap/jit support\n",
    "\n",
    "**Same code everywhere** - develop locally, deploy to cluster\n",
    "\n",
    "```python\n",
    "# That's all you need!\n",
    "dist_info = initialize_distributed()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
