{
 "cells": [
  {
   "cell_type": "raw",
   "id": "39da0275",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Distributed Computing with PtDAlgorithms\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d812b2",
   "metadata": {},
   "source": [
    "\n",
    "**Simple, powerful multi-node parallelization with zero boilerplate**\n",
    "\n",
    "This guide shows you how to use the distributed computing interface to scale your computations from a laptop to 100+ node clusters with just **one line of code**.\n",
    "\n",
    "**Key Features:**\n",
    "- **Automatic SLURM detection** - No manual environment parsing\n",
    "- **Built-in CPU monitoring** - Per-core usage across all nodes\n",
    "- **One-line initialization** - 200+ lines of boilerplate 1 line\n",
    "- **Works everywhere** - Same code runs locally and on clusters\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Quick Start](#quick-start)\n",
    "- [Complete Examples](#complete-examples)\n",
    "- [Configuration Management](#configuration-management)\n",
    "- [SLURM Integration](#slurm-integration)\n",
    "- [Monitoring Distributed Jobs](#monitoring-distributed-jobs)\n",
    "- [Tips and Best Practices](#tips-and-best-practices)\n",
    "- [API Reference](#api-reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27be1a83",
   "metadata": {},
   "source": [
    "\n",
    "### Example 1: Simple Distributed Computation\n",
    "\n",
    "**File:** `distributed_inference_simple.py`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b806819",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers (369961146.py, line 85)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 85\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m| small | 2 | 8 | 16 | 01:00:00 |\u001b[39m\n                           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from ptdalgorithms import Graph, initialize_distributed\n",
    "\n",
    "# ONE LINE - replaces 200+ lines of boilerplate\n",
    "dist_info = initialize_distributed()\n",
    "\n",
    "print(f\"Running on {dist_info.global_device_count} devices\")\n",
    "print(f\"Process {dist_info.process_id}/{dist_info.num_processes}\")\n",
    "\n",
    "# Build your model\n",
    "graph = Graph(1)\n",
    "# ... build graph ...\n",
    "\n",
    "# Your computation automatically uses all devices\n",
    "# via JAX pmap/vmap parallelization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e3b394",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Run locally:**\n",
    "```bash\n",
    "python distributed_inference_simple.py\n",
    "```\n",
    "\n",
    "**Run on SLURM cluster:**\n",
    "```bash\n",
    "# Generate and submit SLURM script in one command\n",
    "sbatch <(python generate_slurm_script.py --profile medium --script distributed_inference_simple.py)\n",
    "```\n",
    "\n",
    "### Example 2: Distributed SVGD Inference with Monitoring\n",
    "\n",
    "**File:** `simple_multinode_example.py`\n",
    "\n",
    "This example shows SVGD (Stein Variational Gradient Descent) inference distributed across multiple nodes with CPU monitoring:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c264941",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Not running under SLURM - using single-node setup\n",
      "[INFO] Configured JAX for 1 CPU devices\n",
      "[INFO] JAX x64 precision enabled\n",
      "[INFO] Single-node setup - no distributed initialization needed\n",
      "[INFO] \n",
      "Distributed Configuration:\n",
      "  Job ID: N/A\n",
      "  Process: 0/1\n",
      "  Coordinator: localhost:12345 (this node)\n",
      "  Local devices: 1\n",
      "  Global devices: 1\n",
      "  Platform: cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: monospace; font-size: 10px; padding: 10px;\"><div style=\"margin-bottom: 12px;\"><div style=\"margin-bottom: 6px; font-size: 11px;\">macbookpro <span style=\"font-weight: normal; color: #666;\">(37%/37% mem)</span></div><div style=\"display: flex; gap: 3px; width: 100%; margin-bottom: 3px;\">\n",
       "                        <div style=\"width: calc((100% - 27px) / 10); min-width: 20px; height: 8px; background: rgba(128, 128, 128, 0.2); border-radius: 2px; overflow: hidden;\" title=\"CPU 0: 100.0% avg\">\n",
       "                            <div style=\"width: 100.0%; height: 100%; background: #4CAF50;\"></div>\n",
       "                        </div>\n",
       "                        \n",
       "                        <div style=\"width: calc((100% - 27px) / 10); min-width: 20px; height: 8px; background: rgba(128, 128, 128, 0.2); border-radius: 2px; overflow: hidden;\" title=\"CPU 1: 100.0% avg\">\n",
       "                            <div style=\"width: 100.0%; height: 100%; background: #4CAF50;\"></div>\n",
       "                        </div>\n",
       "                        \n",
       "                        <div style=\"width: calc((100% - 27px) / 10); min-width: 20px; height: 8px; background: rgba(128, 128, 128, 0.2); border-radius: 2px; overflow: hidden;\" title=\"CPU 2: 28.0% avg\">\n",
       "                            <div style=\"width: 28.0%; height: 100%; background: #4CAF50;\"></div>\n",
       "                        </div>\n",
       "                        \n",
       "                        <div style=\"width: calc((100% - 27px) / 10); min-width: 20px; height: 8px; background: rgba(128, 128, 128, 0.2); border-radius: 2px; overflow: hidden;\" title=\"CPU 3: 20.0% avg\">\n",
       "                            <div style=\"width: 20.0%; height: 100%; background: #4CAF50;\"></div>\n",
       "                        </div>\n",
       "                        \n",
       "                        <div style=\"width: calc((100% - 27px) / 10); min-width: 20px; height: 8px; background: rgba(128, 128, 128, 0.2); border-radius: 2px; overflow: hidden;\" title=\"CPU 4: 20.4% avg\">\n",
       "                            <div style=\"width: 20.4%; height: 100%; background: #4CAF50;\"></div>\n",
       "                        </div>\n",
       "                        \n",
       "                        <div style=\"width: calc((100% - 27px) / 10); min-width: 20px; height: 8px; background: rgba(128, 128, 128, 0.2); border-radius: 2px; overflow: hidden;\" title=\"CPU 5: 33.3% avg\">\n",
       "                            <div style=\"width: 33.3%; height: 100%; background: #4CAF50;\"></div>\n",
       "                        </div>\n",
       "                        \n",
       "                        <div style=\"width: calc((100% - 27px) / 10); min-width: 20px; height: 8px; background: rgba(128, 128, 128, 0.2); border-radius: 2px; overflow: hidden;\" title=\"CPU 6: 10.0% avg\">\n",
       "                            <div style=\"width: 10.0%; height: 100%; background: #4CAF50;\"></div>\n",
       "                        </div>\n",
       "                        \n",
       "                        <div style=\"width: calc((100% - 27px) / 10); min-width: 20px; height: 8px; background: rgba(128, 128, 128, 0.2); border-radius: 2px; overflow: hidden;\" title=\"CPU 7: 3.9% avg\">\n",
       "                            <div style=\"width: 3.9%; height: 100%; background: #4CAF50;\"></div>\n",
       "                        </div>\n",
       "                        \n",
       "                        <div style=\"width: calc((100% - 27px) / 10); min-width: 20px; height: 8px; background: rgba(128, 128, 128, 0.2); border-radius: 2px; overflow: hidden;\" title=\"CPU 8: 2.0% avg\">\n",
       "                            <div style=\"width: 2.0%; height: 100%; background: #4CAF50;\"></div>\n",
       "                        </div>\n",
       "                        \n",
       "                        <div style=\"width: calc((100% - 27px) / 10); min-width: 20px; height: 8px; background: rgba(128, 128, 128, 0.2); border-radius: 2px; overflow: hidden;\" title=\"CPU 9: 0.0% avg\">\n",
       "                            <div style=\"width: 0.0%; height: 100%; background: #4CAF50;\"></div>\n",
       "                        </div>\n",
       "                        </div></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Run SVGD with automatic CPU monitoring\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m CPUMonitor(persist=\u001b[38;5;28;01mTrue\u001b[39;00m, color=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     svgd = SVGD(\u001b[43mmodel\u001b[49m, observed_data, n_particles=num_particles)\n\u001b[32m     13\u001b[39m     results = svgd.fit()\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Monitor persists after completion showing mean usage per core\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from ptdalgorithms import initialize_distributed, CPUMonitor, SVGD\n",
    "from ptdalgorithms.ffi_wrappers import compute_pmf_and_moments_ffi\n",
    "\n",
    "# Initialize distributed computing\n",
    "dist_info = initialize_distributed()\n",
    "\n",
    "# Create particles distributed across all devices\n",
    "num_particles = dist_info.global_device_count * 4  # 4 per device\n",
    "\n",
    "# Run SVGD with automatic CPU monitoring\n",
    "with CPUMonitor(persist=True, color=True):\n",
    "    svgd = SVGD(model, observed_data, n_particles=num_particles)\n",
    "    results = svgd.fit()\n",
    "# Monitor persists after completion showing mean usage per core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678116ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "**Benefits of monitoring:**\n",
    "- See CPU utilization across all allocated cores\n",
    "- Verify efficient parallelization\n",
    "- Detect bottlenecks or idle cores\n",
    "- Track memory usage during inference\n",
    "\n",
    "**Full example:** See `simple_multinode_example.py` for complete code.\n",
    "\n",
    "\n",
    "## Configuration Management\n",
    "\n",
    "The new system uses **YAML files** to separate configuration from code.\n",
    "\n",
    "### Using Predefined Profiles\n",
    "\n",
    "```python\n",
    "from ptdalgorithms.cluster_configs import get_default_config\n",
    "\n",
    "# Available profiles: debug, small, medium, large, production\n",
    "config = get_default_config(\"medium\")\n",
    "\n",
    "print(f\"Nodes: {config.nodes}\")\n",
    "print(f\"CPUs/node: {config.cpus_per_node}\")\n",
    "print(f\"Total devices: {config.total_devices}\")\n",
    "```\n",
    "\n",
    "**Available profiles:**\n",
    "\n",
    "| Profile | Nodes | CPUs/node | Total Devices | Time Limit |\n",
    "|---------|-------|-----------|---------------|------------|\n",
    "| debug | 1 | 4 | 4 | 00:30:00 |\n",
    "| small | 2 | 8 | 16 | 01:00:00 |\n",
    "| medium | 4 | 16 | 64 | 02:00:00 |\n",
    "| large | 8 | 16 | 128 | 04:00:00 |\n",
    "| production | 8 | 32 | 256 | 08:00:00 |\n",
    "\n",
    "### Creating Custom Configurations\n",
    "\n",
    "**File:** `docs/examples/slurm_configs/my_config.yaml`\n",
    "\n",
    "```yaml\n",
    "name: my_custom_config\n",
    "nodes: 4\n",
    "cpus_per_node: 16\n",
    "memory_per_cpu: \"8G\"\n",
    "time_limit: \"03:00:00\"\n",
    "partition: \"compute\"\n",
    "coordinator_port: 12345\n",
    "platform: \"cpu\"\n",
    "\n",
    "env_vars:\n",
    "  JAX_ENABLE_X64: \"1\"\n",
    "  XLA_PYTHON_CLIENT_PREALLOCATE: \"false\"\n",
    "\n",
    "modules_to_load:\n",
    "  - \"python/3.11\"\n",
    "  - \"gcc/11.2.0\"\n",
    "```\n",
    "\n",
    "**Load in Python:**\n",
    "\n",
    "```python\n",
    "from ptdalgorithms.cluster_configs import load_config\n",
    "\n",
    "config = load_config(\"docs/examples/slurm_configs/my_config.yaml\")\n",
    "```\n",
    "\n",
    "\n",
    "## SLURM Integration\n",
    "\n",
    "### Why SLURM Scripts Are Still Needed\n",
    "\n",
    "While `initialize_distributed()` and `CPUMonitor` **automatically detect** the SLURM environment once your code is running, you still need SLURM batch scripts to:\n",
    "\n",
    "1. **Submit jobs** to the SLURM scheduler\n",
    "2. **Allocate resources** (nodes, CPUs, memory, time)\n",
    "3. **Set up the environment** (modules, Python environment)\n",
    "4. **Launch processes** on multiple nodes with `srun`\n",
    "\n",
    "The automatic detection happens **inside** your Python code after SLURM has started the job and set environment variables.\n",
    "\n",
    "### Generating SLURM Scripts\n",
    "\n",
    "The `generate_slurm_script.py` tool creates SLURM submission scripts from configuration files:\n",
    "\n",
    "#### List Available Profiles\n",
    "\n",
    "```bash\n",
    "python generate_slurm_script.py --list-profiles\n",
    "```\n",
    "\n",
    "#### Generate Script from Profile\n",
    "\n",
    "```bash\n",
    "# Generate and save\n",
    "python generate_slurm_script.py \\\n",
    "    --profile medium \\\n",
    "    --script my_script.py \\\n",
    "    --output submit.sh\n",
    "\n",
    "chmod +x submit.sh\n",
    "sbatch submit.sh\n",
    "```\n",
    "\n",
    "#### Quick Submit (One Command!)\n",
    "\n",
    "```bash\n",
    "# Generate and submit in one command\n",
    "sbatch <(python generate_slurm_script.py --profile medium --script my_script.py)\n",
    "```\n",
    "\n",
    "#### Generate from Custom Config\n",
    "\n",
    "```bash\n",
    "python generate_slurm_script.py \\\n",
    "    --config docs/examples/slurm_configs/my_config.yaml \\\n",
    "    --script my_script.py \\\n",
    "    --output submit.sh\n",
    "```\n",
    "\n",
    "### What the Generated Script Does\n",
    "\n",
    "The generated SLURM script automatically:\n",
    "\n",
    "1. **Loads modules** (if specified in config)\n",
    "2. **Activates Python environment** (Pixi or Conda)\n",
    "3. **Sets up JAX coordinator** for distributed computing\n",
    "4. **Configures environment variables**\n",
    "5. **Runs your script** with `srun` for multi-node execution\n",
    "6. **Reports job status** and exit codes\n",
    "\n",
    "**Example generated script:**\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=my_script\n",
    "#SBATCH --nodes=4\n",
    "#SBATCH --ntasks-per-node=1\n",
    "#SBATCH --cpus-per-task=16\n",
    "#SBATCH --time=02:00:00\n",
    "#SBATCH --partition=compute\n",
    "\n",
    "# Load modules\n",
    "module load python/3.11\n",
    "\n",
    "# Activate environment\n",
    "eval \"$(pixi shell-hook)\"\n",
    "\n",
    "# Setup JAX coordinator\n",
    "COORDINATOR_NODE=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)\n",
    "export SLURM_COORDINATOR_ADDRESS=$COORDINATOR_NODE\n",
    "export JAX_COORDINATOR_PORT=12345\n",
    "\n",
    "# Run distributed computation\n",
    "srun --kill-on-bad-exit=1 python my_script.py\n",
    "```\n",
    "\n",
    "\n",
    "## API Reference\n",
    "\n",
    "### Core Functions\n",
    "\n",
    "#### `initialize_distributed()`\n",
    "\n",
    "The main entry point for distributed computing with **automatic SLURM detection**.\n",
    "\n",
    "```python\n",
    "from ptdalgorithms import initialize_distributed\n",
    "\n",
    "dist_info = initialize_distributed(\n",
    "    cpus_per_task=None,      # Auto-detected from SLURM_CPUS_PER_TASK\n",
    "    coordinator_port=12345,  # Port for JAX coordinator\n",
    "    platform=\"cpu\",          # \"cpu\" or \"gpu\"\n",
    "    enable_x64=True          # Enable 64-bit precision\n",
    ")\n",
    "```\n",
    "\n",
    "**Automatic detection includes:**\n",
    "- SLURM job environment (`SLURM_JOB_ID`, `SLURM_PROCID`, etc.)\n",
    "- Number of processes and process rank\n",
    "- Coordinator node from `SLURM_JOB_NODELIST`\n",
    "- CPUs per task allocation\n",
    "- Fallback to single-node mode if not running under SLURM\n",
    "\n",
    "**Returns:** `DistributedConfig` object with:\n",
    "\n",
    "```python\n",
    "dist_info.num_processes        # Total number of processes (nodes)\n",
    "dist_info.process_id           # This process's rank (0 to num_processes-1)\n",
    "dist_info.local_device_count   # Number of devices on this node\n",
    "dist_info.global_device_count  # Total devices across all nodes\n",
    "dist_info.is_coordinator       # True if this is the coordinator (rank 0)\n",
    "dist_info.coordinator_address  # Address of coordinator (\"host:port\")\n",
    "dist_info.job_id              # SLURM job ID (if running under SLURM)\n",
    "```\n",
    "\n",
    "#### `get_default_config(profile)`\n",
    "\n",
    "Get predefined cluster configuration.\n",
    "\n",
    "```python\n",
    "from ptdalgorithms.cluster_configs import get_default_config\n",
    "\n",
    "config = get_default_config(\"medium\")\n",
    "```\n",
    "\n",
    "**Available profiles:** `\"debug\"`, `\"small\"`, `\"medium\"`, `\"large\"`, `\"production\"`\n",
    "\n",
    "#### `load_config(filepath)`\n",
    "\n",
    "Load custom configuration from YAML file.\n",
    "\n",
    "```python\n",
    "from ptdalgorithms.cluster_configs import load_config\n",
    "\n",
    "config = load_config(\"docs/examples/slurm_configs/my_config.yaml\")\n",
    "```\n",
    "\n",
    "#### `CPUMonitor`\n",
    "\n",
    "Context manager for monitoring CPU usage with **automatic SLURM node detection**.\n",
    "\n",
    "```python\n",
    "from ptdalgorithms import CPUMonitor\n",
    "\n",
    "with CPUMonitor(\n",
    "    update_interval=0.5,    # Update every 0.5 seconds\n",
    "    persist=False,          # Clear display after completion\n",
    "    color=False,            # Gray bars (True for color-coded)\n",
    "    summary_table=False     # Show bars (True for table)\n",
    "):\n",
    "    # Your computation here\n",
    "    results = run_computation()\n",
    "```\n",
    "\n",
    "**Automatic detection:**\n",
    "- Detects SLURM nodes from `SLURM_JOB_NODELIST`\n",
    "- Shows only allocated CPUs from `SLURM_CPUS_PER_TASK`\n",
    "- Falls back to local node detection\n",
    "- Works in terminal, Jupyter, and VSCode\n",
    "\n",
    "**Cell magic** (Jupyter/VSCode):\n",
    "```python\n",
    "%%monitor --color --persist --summary\n",
    "# Your computation\n",
    "```\n",
    "\n",
    "### Advanced Functions\n",
    "\n",
    "#### `detect_slurm_environment()`\n",
    "\n",
    "Manually detect SLURM environment variables.\n",
    "\n",
    "```python\n",
    "from ptdalgorithms.distributed_utils import detect_slurm_environment\n",
    "\n",
    "env = detect_slurm_environment()\n",
    "if env:\n",
    "    print(f\"Running SLURM job {env['job_id']}\")\n",
    "    print(f\"Process {env['process_id']}/{env['num_processes']}\")\n",
    "```\n",
    "\n",
    "#### `configure_jax_devices()`\n",
    "\n",
    "Configure JAX device count (called automatically by `initialize_distributed`).\n",
    "\n",
    "```python\n",
    "from ptdalgorithms.distributed_utils import configure_jax_devices\n",
    "\n",
    "configure_jax_devices(num_devices=8, platform=\"cpu\")\n",
    "```\n",
    "\n",
    "#### `initialize_jax_distributed()`\n",
    "\n",
    "Initialize JAX distributed (called automatically by `initialize_distributed`).\n",
    "\n",
    "```python\n",
    "from ptdalgorithms.distributed_utils import initialize_jax_distributed\n",
    "\n",
    "initialize_jax_distributed(\n",
    "    coordinator_address=\"node01:12345\",\n",
    "    num_processes=4,\n",
    "    process_id=0\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "## Comparison: Old vs New\n",
    "\n",
    "### Before (200+ lines)\n",
    "\n",
    "```python\n",
    "# 200+ lines of boilerplate for:\n",
    "# - SLURM detection\n",
    "# - Environment variable setup\n",
    "# - Coordinator configuration\n",
    "# - JAX initialization\n",
    "# - Error handling\n",
    "# - Device configuration\n",
    "```\n",
    "\n",
    "### After (1 line)\n",
    "\n",
    "```python\n",
    "dist_info = initialize_distributed()\n",
    "```\n",
    "\n",
    "### Benefits\n",
    "\n",
    "**80% less code** (200 lines 20 lines)\n",
    "**Reusable** across all projects\n",
    "**Type-safe** with IDE autocomplete\n",
    "**Testable** (can mock SLURM environment)\n",
    "**Maintainable** (one place to update)\n",
    "**Config-driven** (YAML separates config from code)\n",
    "\n",
    "\n",
    "## Monitoring Distributed Jobs\n",
    "\n",
    "PtDAlgorithms includes a built-in CPU monitoring system that **automatically detects SLURM nodes** and displays per-node, per-core CPU usage in real-time.\n",
    "\n",
    "### Automatic Node Detection\n",
    "\n",
    "The monitoring system automatically detects:\n",
    "- **Local execution**: Shows your laptop/workstation\n",
    "- **SLURM jobs**: Detects all allocated nodes from `SLURM_JOB_NODELIST`\n",
    "- **Allocated CPUs**: Filters to show only your job's CPUs from `SLURM_CPUS_PER_TASK`\n",
    "\n",
    "### Example: Monitoring Distributed SVGD\n",
    "\n",
    "```python\n",
    "from ptdalgorithms import initialize_distributed, CPUMonitor\n",
    "import time\n",
    "\n",
    "# Initialize distributed computing (auto-detects SLURM)\n",
    "dist_info = initialize_distributed()\n",
    "\n",
    "# Monitor CPU usage across all allocated nodes\n",
    "with CPUMonitor(update_interval=0.5):\n",
    "    # Your distributed computation here\n",
    "    svgd = SVGD(model, data, n_particles=dist_info.global_device_count * 4)\n",
    "    results = svgd.fit()\n",
    "```\n",
    "\n",
    "**What you'll see:**\n",
    "```\n",
    "node01 45% mem\n",
    "[▓▓▓░░] [▓▓▓░░] [▓▓▓▓░] [▓▓░░░] [▓▓▓░░] [▓▓░░░] [▓▓▓▓░] [▓▓░░░]\n",
    "\n",
    "node02 52% mem\n",
    "[▓▓▓▓░] [▓▓░░░] [▓▓▓░░] [▓▓▓▓░] [▓▓▓░░] [▓▓░░░] [▓▓▓░░] [▓▓▓▓░]\n",
    "```\n",
    "\n",
    "Each bar shows real-time CPU usage for one core. Memory percentage shows current node usage.\n",
    "\n",
    "### Jupyter Notebook Monitoring\n",
    "\n",
    "For Jupyter/VSCode notebooks, use the cell magic:\n",
    "\n",
    "```python\n",
    "%%monitor --color\n",
    "# Your distributed computation\n",
    "svgd = SVGD(model, data, n_particles=64)\n",
    "results = svgd.fit()\n",
    "```\n",
    "\n",
    "**Options:**\n",
    "- `--color` / `-c`: Color-coded bars (green/yellow/red based on usage)\n",
    "- `--persist` / `-p`: Keep display visible after completion with mean usage\n",
    "- `--summary` / `-s`: Show table with CPU percentages instead of bars\n",
    "- `--interval` / `-i`: Update interval in seconds (default: 0.5)\n",
    "\n",
    "### Monitoring Script vs Computation Script\n",
    "\n",
    "Note that the CPU monitor shows usage on the **current node only**. In a multi-node SLURM job:\n",
    "\n",
    "- Each process runs on one node\n",
    "- Each process sees its own node's CPU usage\n",
    "- The monitor automatically detects which node it's running on\n",
    "\n",
    "To see usage across all nodes, check SLURM logs or use cluster monitoring tools.\n",
    "\n",
    "\n",
    "## Tips and Best Practices\n",
    "\n",
    "### 1. Use Coordinator Check for Logging\n",
    "\n",
    "Only the coordinator should print summary information:\n",
    "\n",
    "```python\n",
    "if dist_info.is_coordinator:\n",
    "    print(f\"Starting computation with {dist_info.global_device_count} devices\")\n",
    "```\n",
    "\n",
    "### 2. Distribute Particles Evenly\n",
    "\n",
    "Ensure particles are divisible by device count:\n",
    "\n",
    "```python\n",
    "particles_per_device = 4\n",
    "n_particles = dist_info.global_device_count * particles_per_device\n",
    "```\n",
    "\n",
    "### 3. Use Different Seeds per Process\n",
    "\n",
    "Avoid identical random numbers across processes:\n",
    "\n",
    "```python\n",
    "np.random.seed(42 + dist_info.process_id)\n",
    "```\n",
    "\n",
    "### 4. Test Locally Before SLURM\n",
    "\n",
    "Your code should work both locally and on SLURM:\n",
    "\n",
    "```bash\n",
    "# Test locally first\n",
    "python my_script.py\n",
    "\n",
    "# Then submit to cluster\n",
    "sbatch <(python generate_slurm_script.py --profile small --script my_script.py)\n",
    "```\n",
    "\n",
    "### 5. Monitor Resource Usage\n",
    "\n",
    "Use the built-in CPU monitor to ensure efficient resource utilization:\n",
    "\n",
    "```python\n",
    "with CPUMonitor(persist=True):\n",
    "    # Your computation\n",
    "    results = run_inference()\n",
    "# Shows mean CPU usage after completion\n",
    "```\n",
    "\n",
    "### 6. Create Custom Configs for Your Cluster\n",
    "\n",
    "Different clusters have different partitions, QoS, modules, etc:\n",
    "\n",
    "```yaml\n",
    "# my_cluster.yaml\n",
    "name: my_cluster\n",
    "partition: \"gpu-partition\"  # Your cluster's GPU partition\n",
    "qos: \"high-priority\"         # Your QoS\n",
    "modules_to_load:\n",
    "  - \"cuda/11.8\"              # Your cluster's CUDA module\n",
    "  - \"python/3.11\"\n",
    "```\n",
    "\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Issue: \"Module 'ptdalgorithms' not found\"\n",
    "\n",
    "**Solution:** Ensure the package is installed in your environment:\n",
    "\n",
    "```bash\n",
    "# With Pixi (recommended)\n",
    "pixi install\n",
    "\n",
    "# Or with pip\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "### Issue: \"JAX distributed initialization failed\"\n",
    "\n",
    "**Possible causes:**\n",
    "\n",
    "1. **Coordinator node unreachable**: Check network connectivity between nodes\n",
    "2. **Port already in use**: Try a different `coordinator_port`\n",
    "3. **Firewall blocking**: Ensure port is open for inter-node communication\n",
    "\n",
    "**Debug with:**\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ['JAX_LOG_LEVEL'] = 'DEBUG'\n",
    "dist_info = initialize_distributed()\n",
    "```\n",
    "\n",
    "### Issue: \"SLURM environment not detected\"\n",
    "\n",
    "**Check SLURM variables:**\n",
    "\n",
    "```bash\n",
    "echo $SLURM_JOB_ID\n",
    "echo $SLURM_PROCID\n",
    "echo $SLURM_NTASKS\n",
    "```\n",
    "\n",
    "If not running under SLURM, the code falls back to single-node mode (this is expected).\n",
    "\n",
    "### Issue: \"Particles not evenly distributed\"\n",
    "\n",
    "Ensure `n_particles` is divisible by `global_device_count`:\n",
    "\n",
    "```python\n",
    "# Good\n",
    "n_particles = dist_info.global_device_count * 4  # Exactly 4 per device\n",
    "\n",
    "# Bad\n",
    "n_particles = 37  # Won't divide evenly across devices\n",
    "```\n",
    "\n",
    "\n",
    "## Complete Workflow Example\n",
    "\n",
    "Here's a complete workflow from development to production:\n",
    "\n",
    "### 1. Develop Locally\n",
    "\n",
    "```bash\n",
    "# Create your script\n",
    "vim my_inference.py\n",
    "\n",
    "# Test locally (single node)\n",
    "python my_inference.py\n",
    "```\n",
    "\n",
    "### 2. Create Custom Config\n",
    "\n",
    "```bash\n",
    "# Create config for your cluster\n",
    "vim slurm_configs/my_cluster.yaml\n",
    "```\n",
    "\n",
    "### 3. Test on Small Scale\n",
    "\n",
    "```bash\n",
    "# Test with small profile (2 nodes)\n",
    "sbatch <(python generate_slurm_script.py --profile small --script my_inference.py)\n",
    "\n",
    "# Monitor\n",
    "squeue -u $USER\n",
    "```\n",
    "\n",
    "### 4. Scale to Production\n",
    "\n",
    "```bash\n",
    "# Run on full cluster\n",
    "sbatch <(python generate_slurm_script.py --profile production --script my_inference.py)\n",
    "```\n",
    "\n",
    "### 5. Monitor Results\n",
    "\n",
    "```bash\n",
    "# Check logs\n",
    "tail -f logs/my_inference_*.out\n",
    "\n",
    "# Check errors\n",
    "tail -f logs/my_inference_*.err\n",
    "```\n",
    "\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- **Full examples:** See `examples/` directory\n",
    "  - `distributed_inference_simple.py` - Basic distributed computation\n",
    "  - `simple_multinode_example.py` - SVGD inference example\n",
    "  - `distributed_svgd_example.py` - Advanced SVGD with synthetic data\n",
    "\n",
    "- **Configuration examples:** See `docs/examples/slurm_configs/`\n",
    "  - `debug.yaml` - Quick testing\n",
    "  - `small_cluster.yaml` - Development\n",
    "  - `medium_cluster.yaml` - Standard jobs\n",
    "  - `production.yaml` - Large-scale inference\n",
    "  - `gpu_cluster.yaml` - GPU acceleration\n",
    "\n",
    "- **SLURM guide:** See `SLURM_MULTINODE_GUIDE.md` for advanced SLURM topics\n",
    "\n",
    "\n",
    "## Getting Help\n",
    "\n",
    "If you encounter issues:\n",
    "\n",
    "1. Check the examples in `examples/` directory\n",
    "2. Review this guide\n",
    "3. Check `SLURM_MULTINODE_GUIDE.md` for advanced topics\n",
    "4. Open an issue on GitHub\n",
    "\n",
    "\n",
    "**Happy distributed computing! 🚀**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f9eab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
