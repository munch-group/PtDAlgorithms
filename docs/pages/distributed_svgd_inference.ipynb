{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed SVGD Inference\n",
    "\n",
    "This guide demonstrates how to perform Bayesian inference using Stein Variational Gradient Descent (SVGD) distributed across multiple nodes and devices.\n",
    "\n",
    "## What is SVGD?\n",
    "\n",
    "SVGD is a powerful algorithm for Bayesian inference that:\n",
    "\n",
    "- Finds the posterior distribution p(θ | data)\n",
    "- Uses a set of particles to approximate the posterior\n",
    "- Supports automatic differentiation through JAX\n",
    "- Scales efficiently to multiple devices\n",
    "\n",
    "## Overview\n",
    "\n",
    "We'll demonstrate:\n",
    "\n",
    "1. Building a parameterized coalescent model\n",
    "2. Distributing SVGD particles across devices\n",
    "3. Running Bayesian inference at scale\n",
    "4. Analyzing posterior distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, initialize distributed computing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from ptdalgorithms import Graph, initialize_distributed, SVGD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize distributed computing\n",
    "dist_info = initialize_distributed(\n",
    "    platform=\"cpu\",\n",
    "    enable_x64=True\n",
    ")\n",
    "\n",
    "if dist_info.is_coordinator:\n",
    "    print(\"Distributed SVGD Inference\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Processes: {dist_info.num_processes}\")\n",
    "    print(f\"Total devices: {dist_info.global_device_count}\")\n",
    "    print(f\"Local devices: {dist_info.local_device_count}\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Parameterized Model\n",
    "\n",
    "We'll use a coalescent model - this models the time to most recent common ancestor (TMRCA) for a sample of DNA sequences.\n",
    "\n",
    "### The Coalescent Process\n",
    "\n",
    "The coalescent is a stochastic model of genealogy that:\n",
    "\n",
    "- Starts with n sampled lineages\n",
    "- Lineages coalesce (merge) backwards in time\n",
    "- Coalescent rate depends on effective population size\n",
    "- Parameter θ = 4Nμ (population size × mutation rate)\n",
    "\n",
    "### Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_coalescent_model(nr_samples=4):\n",
    "    \"\"\"\n",
    "    Build a parameterized coalescent model.\n",
    "    \n",
    "    The model has one parameter θ that scales all coalescent rates.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    nr_samples : int\n",
    "        Number of sampled sequences\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Graph\n",
    "        Parameterized coalescent graph\n",
    "    \"\"\"\n",
    "    def coalescent_callback(state, nr_samples=nr_samples):\n",
    "        \"\"\"Define coalescent transitions.\"\"\"\n",
    "        if len(state) == 0:\n",
    "            # Initial: all samples in separate lineages\n",
    "            return [(np.array([nr_samples]), 1.0, [1.0])]\n",
    "        \n",
    "        if state[0] > 1:\n",
    "            # Coalescent event: n → n-1 lineages\n",
    "            n = state[0]\n",
    "            rate = n * (n - 1) / 2  # Rate = n choose 2\n",
    "            return [(np.array([n - 1]), 0.0, [rate])]\n",
    "        \n",
    "        # Absorbing state (MRCA reached)\n",
    "        return []\n",
    "    \n",
    "    # Build parameterized graph\n",
    "    graph = Graph(\n",
    "        callback=coalescent_callback,\n",
    "        parameterized=True,\n",
    "        nr_samples=nr_samples\n",
    "    )\n",
    "    \n",
    "    return graph\n",
    "\n",
    "# Build model\n",
    "nr_samples = 5\n",
    "graph = build_coalescent_model(nr_samples=nr_samples)\n",
    "\n",
    "if dist_info.is_coordinator:\n",
    "    print(f\"\\nBuilt coalescent model:\")\n",
    "    print(f\"  Samples: {nr_samples}\")\n",
    "    print(f\"  States: {graph.vertices_length()}\")\n",
    "    print(f\"  Parameters: 1 (θ)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Synthetic Data\n",
    "\n",
    "For demonstration, we'll generate synthetic data from a known parameter value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True parameter value\n",
    "true_theta = jnp.array([1.0])\n",
    "\n",
    "# Generate evaluation times\n",
    "evaluation_times = jnp.linspace(0.1, 5.0, 30)\n",
    "\n",
    "# Create JAX-compatible model for true parameter\n",
    "model_fn = Graph.pmf_from_graph(graph, discrete=False)\n",
    "\n",
    "# Generate true PDF\n",
    "true_pdf = model_fn(true_theta, evaluation_times)\n",
    "\n",
    "# Add small noise to simulate observations\n",
    "np.random.seed(42)\n",
    "noise = np.random.normal(0, 0.01 * float(jnp.max(true_pdf)), size=true_pdf.shape)\n",
    "observed_pdf = true_pdf + noise\n",
    "observed_pdf = jnp.maximum(observed_pdf, 1e-10)  # Ensure positive\n",
    "\n",
    "if dist_info.is_coordinator:\n",
    "    print(f\"\\nGenerated synthetic data:\")\n",
    "    print(f\"  True θ: {float(true_theta[0]):.3f}\")\n",
    "    print(f\"  Observations: {len(observed_pdf)}\")\n",
    "    print(f\"  Time range: [{float(evaluation_times[0]):.2f}, {float(evaluation_times[-1]):.2f}]\")\n",
    "    \n",
    "    # Visualize data\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(evaluation_times, true_pdf, 'b-', linewidth=2, label='True PDF', alpha=0.7)\n",
    "    plt.scatter(evaluation_times, observed_pdf, color='red', s=30, \n",
    "                label='Observed (with noise)', alpha=0.6)\n",
    "    plt.xlabel('Time', fontsize=12)\n",
    "    plt.ylabel('Probability Density', fontsize=12)\n",
    "    plt.title(f'Coalescent Model: True Parameter θ = {float(true_theta[0]):.2f}', fontsize=14)\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Distributed SVGD\n",
    "\n",
    "Now we'll configure SVGD to distribute particles across all available devices:\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Particles**: Each particle represents a sample from the posterior\n",
    "- **Distribution**: Particles are automatically distributed across devices\n",
    "- **Gradient computation**: Computed in parallel using JAX\n",
    "- **Communication**: JAX handles inter-device communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure SVGD\n",
    "n_particles = dist_info.global_device_count * 8  # 8 particles per device\n",
    "n_iterations = 300\n",
    "learning_rate = 0.01\n",
    "\n",
    "if dist_info.is_coordinator:\n",
    "    print(f\"\\nSVGD Configuration:\")\n",
    "    print(f\"  Total particles: {n_particles}\")\n",
    "    print(f\"  Particles per device: {n_particles // dist_info.global_device_count}\")\n",
    "    print(f\"  Iterations: {n_iterations}\")\n",
    "    print(f\"  Learning rate: {learning_rate}\")\n",
    "\n",
    "# Initialize particles\n",
    "np.random.seed(42 + dist_info.process_id)\n",
    "theta_init = np.random.uniform(0.5, 1.5, size=(n_particles, 1))\n",
    "\n",
    "# Define prior (weak Gaussian)\n",
    "def log_prior(theta):\n",
    "    \"\"\"Gaussian prior centered at 1.0 with std 2.0\"\"\"\n",
    "    return -0.5 * jnp.sum((theta - 1.0)**2 / (2.0**2))\n",
    "\n",
    "if dist_info.is_coordinator:\n",
    "    print(f\"\\nInitial particle statistics:\")\n",
    "    print(f\"  Mean: {np.mean(theta_init):.3f}\")\n",
    "    print(f\"  Std: {np.std(theta_init):.3f}\")\n",
    "    print(f\"  Range: [{np.min(theta_init):.3f}, {np.max(theta_init):.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running SVGD Inference\n",
    "\n",
    "Now we run SVGD. The computation is automatically distributed across all devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SVGD instance\n",
    "svgd = SVGD(\n",
    "    model=lambda theta: model_fn(theta, evaluation_times),\n",
    "    observed_data=observed_pdf,\n",
    "    prior=log_prior,\n",
    "    n_particles=n_particles,\n",
    "    n_iterations=n_iterations,\n",
    "    learning_rate=learning_rate,\n",
    "    kernel='rbf_median',\n",
    "    theta_init=theta_init,\n",
    "    theta_dim=1,\n",
    "    seed=42,\n",
    "    verbose=dist_info.is_coordinator  # Only coordinator prints\n",
    ")\n",
    "\n",
    "# Run inference (this may take a minute)\n",
    "if dist_info.is_coordinator:\n",
    "    print(\"\\nRunning SVGD inference...\")\n",
    "    print(\"(This may take a minute)\\n\")\n",
    "\n",
    "svgd.fit(return_history=True)\n",
    "\n",
    "# Get results\n",
    "results = svgd.get_results()\n",
    "\n",
    "if dist_info.is_coordinator:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"INFERENCE COMPLETE\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Results\n",
    "\n",
    "Let's examine the posterior distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dist_info.is_coordinator:\n",
    "    # Extract results\n",
    "    posterior_samples = results['particles']\n",
    "    posterior_mean = results['theta_mean']\n",
    "    posterior_std = results['theta_std']\n",
    "    \n",
    "    # Compute credible interval\n",
    "    lower = posterior_mean[0] - 1.96 * posterior_std[0]\n",
    "    upper = posterior_mean[0] + 1.96 * posterior_std[0]\n",
    "    \n",
    "    print(f\"\\nPosterior Statistics:\")\n",
    "    print(f\"  True θ:           {float(true_theta[0]):.4f}\")\n",
    "    print(f\"  Posterior mean:   {posterior_mean[0]:.4f}\")\n",
    "    print(f\"  Posterior std:    {posterior_std[0]:.4f}\")\n",
    "    print(f\"  95% Credible Int: [{lower:.4f}, {upper:.4f}]\")\n",
    "    \n",
    "    # Check if true value is in credible interval\n",
    "    if lower <= true_theta[0] <= upper:\n",
    "        print(f\"\\n✓ True parameter within 95% credible interval\")\n",
    "    else:\n",
    "        print(f\"\\n⚠ True parameter outside 95% credible interval\")\n",
    "        print(f\"  (Try increasing iterations or particles)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Posterior Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dist_info.is_coordinator:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Left: Posterior histogram\n",
    "    ax = axes[0]\n",
    "    ax.hist(posterior_samples[:, 0], bins=30, density=True, \n",
    "            alpha=0.7, color='blue', edgecolor='black')\n",
    "    ax.axvline(true_theta[0], color='red', linestyle='--', \n",
    "               linewidth=2, label=f'True θ = {float(true_theta[0]):.3f}')\n",
    "    ax.axvline(posterior_mean[0], color='green', linestyle='-', \n",
    "               linewidth=2, label=f'Posterior mean = {posterior_mean[0]:.3f}')\n",
    "    ax.axvspan(lower, upper, alpha=0.2, color='green', \n",
    "               label='95% Credible Interval')\n",
    "    ax.set_xlabel('θ', fontsize=12)\n",
    "    ax.set_ylabel('Density', fontsize=12)\n",
    "    ax.set_title('Posterior Distribution', fontsize=14)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Right: Convergence (if history available)\n",
    "    ax = axes[1]\n",
    "    if 'history' in results and results['history'] is not None:\n",
    "        history = results['history']\n",
    "        # Plot particle trajectories (sample a few)\n",
    "        n_plot = min(20, n_particles)\n",
    "        for i in range(n_plot):\n",
    "            ax.plot(history[:, i, 0], alpha=0.3, color='blue', linewidth=0.5)\n",
    "        ax.axhline(true_theta[0], color='red', linestyle='--', \n",
    "                   linewidth=2, label=f'True θ')\n",
    "        ax.set_xlabel('Iteration', fontsize=12)\n",
    "        ax.set_ylabel('θ', fontsize=12)\n",
    "        ax.set_title('SVGD Particle Convergence', fontsize=14)\n",
    "        ax.legend(fontsize=10)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'History not saved', \n",
    "                ha='center', va='center', fontsize=14)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Predictions\n",
    "\n",
    "Let's compare the posterior predictive distribution with the true model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dist_info.is_coordinator:\n",
    "    # Posterior predictive: sample from posterior\n",
    "    n_posterior_samples = 50\n",
    "    sample_indices = np.random.choice(\n",
    "        len(posterior_samples), \n",
    "        size=n_posterior_samples, \n",
    "        replace=False\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot posterior predictive samples\n",
    "    for idx in sample_indices:\n",
    "        theta_sample = jnp.array([posterior_samples[idx, 0]])\n",
    "        pred_pdf = model_fn(theta_sample, evaluation_times)\n",
    "        plt.plot(evaluation_times, pred_pdf, 'b-', alpha=0.1, linewidth=1)\n",
    "    \n",
    "    # Plot true PDF\n",
    "    plt.plot(evaluation_times, true_pdf, 'r-', linewidth=3, \n",
    "             label=f'True PDF (θ={float(true_theta[0]):.3f})', alpha=0.8)\n",
    "    \n",
    "    # Plot observed data\n",
    "    plt.scatter(evaluation_times, observed_pdf, color='black', s=40,\n",
    "                label='Observed data', alpha=0.6, zorder=5)\n",
    "    \n",
    "    # Plot posterior mean prediction\n",
    "    mean_pred = model_fn(jnp.array([posterior_mean[0]]), evaluation_times)\n",
    "    plt.plot(evaluation_times, mean_pred, 'g--', linewidth=2,\n",
    "             label=f'Posterior mean (θ={posterior_mean[0]:.3f})', alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Time', fontsize=12)\n",
    "    plt.ylabel('Probability Density', fontsize=12)\n",
    "    plt.title('Posterior Predictive Distribution', fontsize=14)\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n✓ Plotted {n_posterior_samples} posterior predictive samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis\n",
    "\n",
    "Let's see how the computation scaled across devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dist_info.is_coordinator:\n",
    "    print(\"\\nPerformance Summary:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Devices used: {dist_info.global_device_count}\")\n",
    "    print(f\"Total particles: {n_particles}\")\n",
    "    print(f\"Particles per device: {n_particles // dist_info.global_device_count}\")\n",
    "    print(f\"Iterations: {n_iterations}\")\n",
    "    print(f\"Total evaluations: {n_particles * n_iterations:,}\")\n",
    "    print(\"\\nScaling benefits:\")\n",
    "    print(f\"  With 1 device: ~{n_particles}x slower\")\n",
    "    print(f\"  With {dist_info.global_device_count} devices: current performance\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running on SLURM Clusters\n",
    "\n",
    "This same code can run on a SLURM cluster without modifications!\n",
    "\n",
    "### Save this notebook as a Python script\n",
    "\n",
    "```bash\n",
    "# Convert notebook to Python script\n",
    "jupyter nbconvert --to python distributed_svgd_inference.ipynb\n",
    "```\n",
    "\n",
    "### Submit to SLURM\n",
    "\n",
    "```bash\n",
    "# Quick submit with default profile\n",
    "sbatch <(python generate_slurm_script.py \\\n",
    "         --profile medium \\\n",
    "         --script distributed_svgd_inference.py)\n",
    "\n",
    "# Or create custom config and submit\n",
    "python generate_slurm_script.py \\\n",
    "    --config my_config.yaml \\\n",
    "    --script distributed_svgd_inference.py \\\n",
    "    --output submit.sh\n",
    "sbatch submit.sh\n",
    "```\n",
    "\n",
    "### Monitor job\n",
    "\n",
    "```bash\n",
    "# Check job status\n",
    "squeue -u $USER\n",
    "\n",
    "# View output\n",
    "tail -f logs/distributed_svgd_inference_*.out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Custom Models\n",
    "\n",
    "You can easily adapt this workflow for your own models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Your custom model\n",
    "def my_custom_model(nr_states=10):\n",
    "    \"\"\"\n",
    "    Replace this with your own model builder.\n",
    "    \"\"\"\n",
    "    def callback(state, param1=1.0, param2=2.0):\n",
    "        # Your model logic here\n",
    "        # Return list of (next_state, weight, edge_state) tuples\n",
    "        pass\n",
    "    \n",
    "    return Graph(\n",
    "        callback=callback,\n",
    "        parameterized=True,\n",
    "        param1=1.0,\n",
    "        param2=2.0\n",
    "    )\n",
    "\n",
    "# Then use the same SVGD workflow:\n",
    "# 1. Build model\n",
    "# 2. Create JAX function with pmf_from_graph()\n",
    "# 3. Run SVGD with your observed data\n",
    "# 4. Analyze posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "✅ **Distributed SVGD** scales seamlessly from laptop to cluster\n",
    "\n",
    "✅ **No code changes** needed between local and SLURM execution\n",
    "\n",
    "✅ **Automatic parallelization** across all available devices\n",
    "\n",
    "✅ **JAX integration** enables gradients and JIT compilation\n",
    "\n",
    "✅ **Flexible models** - works with any parameterized graph\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **[SLURM Cluster Setup](slurm_cluster_setup.ipynb)** - Configure cluster resources\n",
    "- **[Distributed Computing Basics](distributed_computing_basics.ipynb)** - Learn the fundamentals\n",
    "- **[API Reference](../api/index.html)** - Complete API documentation\n",
    "\n",
    "## Summary\n",
    "\n",
    "This example demonstrated:\n",
    "\n",
    "1. Building parameterized coalescent models\n",
    "2. Distributing SVGD particles across devices\n",
    "3. Running Bayesian inference at scale\n",
    "4. Analyzing and visualizing posterior distributions\n",
    "5. Deploying to SLURM clusters\n",
    "\n",
    "All with just:\n",
    "\n",
    "```python\n",
    "dist_info = initialize_distributed()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
