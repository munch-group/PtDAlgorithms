---
title: SVGD Inference
---

## Overview

The `svgd.ipynb` notebook provides comprehensive, tested examples of Stein Variational Gradient Descent (SVGD) inference with PtDAlgorithms.

## What's Covered

### 1. Basic SVGD (Part 1)
- Building parameterized coalescent models
- Converting to JAX-compatible functions
- Running SVGD inference
- Two methods: `Graph.svgd()` class method and `SVGD` class
- Visualizing posterior distributions

### 2. Discrete Models (Part 2)
- Discretizing continuous models
- SVGD with discrete phase-type distributions (DPH)
- PMF evaluation over jump counts

### 3. Multi-CPU Parallelization (Part 3)
- Automatic resource detection with `init_parallel()`
- Distributing particles across CPUs
- Performance comparison
- Manual CPU specification

### 4. Distributed Computing (Part 4)
- SLURM multi-node setup
- Automatic SLURM detection
- Example batch scripts
- Multi-node particle distribution

### 5. Advanced Options (Part 5)
- **Custom priors**: Uniform, Gamma, custom log-prior functions
- **Kernel options**: 'rbf_median', 'rbf_adaptive', fixed bandwidth
- **Hyperparameter tuning**: Learning rate, particles, iterations
- **Moment regularization**: Stabilizing inference with moment matching

### 6. Batching and Sharding (Part 6)
- Understanding particle distribution across devices
- Manual device placement (advanced)
- Batch size considerations
- Memory and performance guidelines

### 7. Troubleshooting (Part 7)
- Common issues and solutions
- Best practices checklist
- Diagnostic functions
- Convergence checking

## Quick Start

```python
import ptdalgorithms as pta
import jax.numpy as jnp

# 1. Initialize parallel computing
pta.init_parallel()

# 2. Build parameterized model
def coalescent_callback(state, nr_samples=3):
    if len(state) == 0:
        return [(np.array([nr_samples]), 1.0, [1.0])]
    if state[0] > 1:
        n = state[0]
        rate = n * (n - 1) / 2
        return [(np.array([n - 1]), 0.0, [rate])]
    return []

graph = pta.Graph(callback=coalescent_callback, parameterized=True, nr_samples=4)
model = pta.Graph.pmf_from_graph(graph, discrete=False)

# 3. Generate or load observed data
observed_times = jnp.linspace(0.1, 4.0, 20)
observed_pdf = model(true_theta, observed_times)  # Your data here

# 4. Run SVGD inference
results = pta.Graph.svgd(
    model=model,
    observed_data=observed_pdf,
    theta_dim=1,
    n_particles=50,
    n_iterations=1000,
    learning_rate=0.01
)

# 5. Analyze results
print(f"Posterior mean: {results['theta_mean']}")
print(f"Posterior std: {results['theta_std']}")
```

## Key Features

### Automatic Parallelization
```python
# Automatically uses all available CPUs
config = pta.init_parallel()

# SVGD automatically parallelizes across devices
results = pta.Graph.svgd(model, data, theta_dim=1, n_particles=50)
```

### Multiple Approaches
```python
# Method 1: Class method (simple)
results = pta.Graph.svgd(model, data, theta_dim=1)

# Method 2: SVGD class (more control)
svgd = pta.SVGD(model, data, theta_dim=1)
svgd.fit()
svgd.plot_posterior()  # Built-in diagnostics
```

### Custom Priors
```python
def log_prior_gamma(theta):
    return jnp.sum(2.0 * jnp.log(theta) - 2.0 * theta)

results = pta.Graph.svgd(
    model=model,
    observed_data=data,
    prior=log_prior_gamma,  # Custom prior
    theta_dim=1
)
```

### Moment Regularization
```python
# Create model with moments
model = pta.Graph.pmf_and_moments_from_graph(graph, nr_moments=2)

# Run regularized SVGD
svgd = pta.SVGD(model, observed_pmf, theta_dim=1)
svgd.fit_regularized(
    observed_times=observation_times,
    regularization=1.0  # Strength of moment matching
)
```

## Running on SLURM

The same code runs on SLURM clusters without modifications:

```bash
# Convert notebook to script
jupyter nbconvert --to python svgd.ipynb

# Submit to SLURM
sbatch submit_svgd.sh
```

Example SLURM script:
```bash
#!/bin/bash
#SBATCH --nodes=4
#SBATCH --ntasks=4
#SBATCH --cpus-per-task=8
#SBATCH --time=02:00:00

srun python svgd_script.py
```

The code automatically detects:
- `SLURM_JOB_ID` (running in SLURM)
- `SLURM_CPUS_PER_TASK` (CPUs per node)
- `SLURM_NTASKS` (number of nodes)

And initializes distributed JAX accordingly.

## Common Patterns

### Pattern 1: Single Machine, Multiple CPUs
```python
import ptdalgorithms as pta

# Automatic detection
config = pta.init_parallel()  # Uses all CPUs

# Run SVGD
results = pta.Graph.svgd(model, data, theta_dim=1, n_particles=50)
```

### Pattern 2: Interactive Development
```python
# Cell 1: Setup
config = pta.init_parallel()

# Cell 2: Build model
graph = pta.Graph(...)
model = pta.Graph.pmf_from_graph(graph)

# Cell 3: Quick test
with pta.disable_parallel():
    test_result = pta.Graph.svgd(model, data, n_particles=10, n_iterations=50)

# Cell 4: Full run
results = pta.Graph.svgd(model, data, n_particles=50, n_iterations=1000)
```

### Pattern 3: Hyperparameter Tuning
```python
learning_rates = [0.001, 0.01, 0.1]
results_list = []

for lr in learning_rates:
    results = pta.Graph.svgd(
        model, data, theta_dim=1,
        learning_rate=lr
    )
    results_list.append(results)

# Compare results
for lr, res in zip(learning_rates, results_list):
    print(f"LR={lr}: mean={res['theta_mean'][0]:.4f}")
```

## Testing and Validation

Run the validation script to ensure everything works:

```bash
python validate_svgd_notebook.py
```

This checks:
- ✓ Model building and evaluation
- ✓ Context managers
- ✓ Discrete model creation
- ✓ Parallel initialization
- ✓ API accessibility

Full SVGD convergence tests are in the notebook itself.

## Troubleshooting

### Issue 1: Poor Convergence
**Symptoms**: Posterior mean far from true value

**Solutions**:
- Increase `n_iterations` (try 1000-2000)
- Adjust `learning_rate` (try 0.001, 0.01, 0.1)
- Increase `n_particles` (try 50-100)
- Check if data is informative

### Issue 2: NaN or Inf
**Symptoms**: NaN in gradients

**Solutions**:
- Reduce learning rate
- Add epsilon: `jnp.log(model_output + 1e-10)`
- Check model for numerical issues

### Issue 3: Out of Memory
**Symptoms**: OOM error

**Solutions**:
- Reduce `n_particles`
- Subsample observed data
- Use fewer observation points

### Issue 4: Slow Performance
**Symptoms**: Takes too long

**Solutions**:
- Call `pta.init_parallel()` first
- Reduce `n_particles` or `n_iterations`
- Use SLURM cluster for large problems

### Issue 5: JAX Device Issues
**Symptoms**: Not using multiple CPUs

**Solutions**:
- Call `pta.init_parallel()` **before** other imports
- Verify: `jax.devices()` shows multiple devices
- Check: XLA_FLAGS set correctly

## Best Practices

✓ **Always** call `pta.init_parallel()` at the start
✓ **Test** model before SVGD: `model(test_theta, test_times)`
✓ **Start small**: 30 particles, 500 iterations, then scale up
✓ **Monitor**: Use `return_history=True` and plot convergence
✓ **Validate**: Test on synthetic data with known parameters
✓ **Document**: Record hyperparameters and any issues

## Performance Guidelines

| Scenario | Particles | Iterations | Time (approx) |
|----------|-----------|------------|---------------|
| Quick test | 10-20 | 100-200 | <1 minute |
| Development | 30-50 | 500-1000 | 2-5 minutes |
| Production | 50-100 | 1000-2000 | 10-30 minutes |
| High accuracy | 100+ | 2000+ | 1+ hours |

**Scaling**:
- 1 CPU: baseline
- 8 CPUs: ~5-7x faster
- SLURM 4 nodes × 8 CPUs: ~20-25x faster

## Additional Resources

- [PtDAlgorithms Documentation](https://docs.ptdalgorithms.com)
- [SVGD Paper](https://arxiv.org/abs/1608.04471)
- [JAX Documentation](https://jax.readthedocs.io)
- Other example notebooks in `docs/examples/python/`

## Support

For issues or questions:
1. Check the troubleshooting section in the notebook
2. Run `validate_svgd_notebook.py` to test your setup
3. Consult the documentation
4. Open an issue on GitHub

## Version Requirements

- Python 3.8+
- JAX 0.4.0+
- PtDAlgorithms 0.20.0+
- NumPy, Matplotlib (optional for visualization)

Install with:
```bash
pip install ptdalgorithms[jax]
```

Or with pixi:
```bash
pixi add ptdalgorithms jax jaxlib
```

---

**Last Updated**: 2025-10-11
**Notebook**: `docs/examples/python/svgd.ipynb`
**Validation**: `validate_svgd_notebook.py`
