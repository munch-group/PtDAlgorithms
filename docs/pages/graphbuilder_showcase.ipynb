{
 "cells": [
  {
   "cell_type": "raw",
   "id": "42931d12",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Comprehensive GraphBuilder & JAX FFI Showcase\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b0f98b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GraphBuilder & JAX FFI Comprehensive Showcase\n",
      "================================================================================\n",
      "JAX version: 0.7.2\n",
      "JAX devices: [CpuDevice(id=0)]\n",
      "JAX x64 enabled: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Comprehensive GraphBuilder & JAX FFI Showcase\n",
    "\n",
    "This script demonstrates all features of the new parameterized GraphBuilder\n",
    "and JAX FFI integration for efficient phase-type distribution computation.\n",
    "\n",
    "Features demonstrated:\n",
    "1. Direct GraphBuilder usage (pybind11)\n",
    "2. JAX FFI wrappers with pure_callback\n",
    "3. JIT compilation\n",
    "4. vmap batching (for multi-parameter inference)\n",
    "5. Combined PMF + moments (for SVGD with regularization)\n",
    "6. Performance comparison\n",
    "7. Real-world SVGD-like workflow\n",
    "\n",
    "Requirements:\n",
    "- JAX with x64 enabled\n",
    "- GraphBuilder module compiled with parameterized support\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import config\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Enable 64-bit types in JAX (required for C++ float64 compatibility)\n",
    "config.update('jax_enable_x64', True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GraphBuilder & JAX FFI Comprehensive Showcase\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "print(f\"JAX x64 enabled: {config.jax_enable_x64}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "267fec2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Part 1: Creating Parameterized Erlang Distribution\n",
      "================================================================================\n",
      "Distribution: Erlang(n=3, λ=theta[0])\n",
      "Number of states: 5\n",
      "Number of parameters: 1\n",
      "Structure JSON length: 213 bytes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# Part 1: Create a Parameterized Phase-Type Distribution\n",
    "# ============================================================================\n",
    "\n",
    "def create_erlang_distribution(num_stages: int = 3) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Create a parameterized Erlang distribution.\n",
    "\n",
    "    Erlang(n, λ) = sum of n independent Exp(λ) random variables\n",
    "\n",
    "    Structure:\n",
    "    - start -> s0 -> s1 -> ... -> s_{n-1} -> absorbing\n",
    "    - Each transition has rate λ (parameterized by theta[0])\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_stages : int\n",
    "        Number of exponential stages\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        JSON-serializable dictionary representing the graph structure\n",
    "    \"\"\"\n",
    "    # State vectors: Each stage needs a unique state value\n",
    "    # - Index 0: start state [0] (not absorbing)\n",
    "    # - Indices 1 to num_stages: intermediate stages [2], [3], ..., [num_stages+1] (not absorbing, unique values)\n",
    "    # - Index num_stages+1: final absorbing state [1]\n",
    "    states = [[0]] + [[i+1] for i in range(1, num_stages + 1)] + [[1]]\n",
    "\n",
    "    # Regular edges: none (all edges are parameterized)\n",
    "    edges = []\n",
    "\n",
    "    # Start edge: start -> s0 with weight 1.0\n",
    "    start_edges = [[1, 1.0]]  # to state index 1 (s0)\n",
    "\n",
    "    # Parameterized edges: s_i -> s_{i+1} with rate = theta[0]\n",
    "    param_edges = []\n",
    "    for i in range(1, num_stages + 1):\n",
    "        param_edges.append([i, i+1, 1.0])  # from stage i to stage i+1\n",
    "\n",
    "    # Final transition from s_{num_stages} to absorbing is included in the loop above\n",
    "\n",
    "    # No parameterized start edges\n",
    "    start_param_edges = []\n",
    "\n",
    "    return {\n",
    "        \"states\": states,\n",
    "        \"edges\": edges,\n",
    "        \"start_edges\": start_edges,\n",
    "        \"param_edges\": param_edges,\n",
    "        \"start_param_edges\": start_param_edges,\n",
    "        \"param_length\": 1,\n",
    "        \"state_length\": 1,\n",
    "        \"n_vertices\": len(states)\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Part 1: Creating Parameterized Erlang Distribution\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "erlang_structure = create_erlang_distribution(num_stages=3)\n",
    "erlang_json = json.dumps(erlang_structure)\n",
    "\n",
    "print(f\"Distribution: Erlang(n=3, λ=theta[0])\")\n",
    "print(f\"Number of states: {erlang_structure['n_vertices']}\")\n",
    "print(f\"Number of parameters: {erlang_structure['param_length']}\")\n",
    "print(f\"Structure JSON length: {len(erlang_json)} bytes\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9913b6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Part 2: Direct GraphBuilder Usage (pybind11)\n",
      "================================================================================\n",
      "[CpuDevice(id=0)]\n",
      "GraphBuilder created:\n",
      "  - param_length: 1\n",
      "  - state_length: 1\n",
      "  - vertices_length: 5\n",
      "\n",
      "Computing PDF for different rate parameters...\n",
      "  λ=0.5: peak at t=4.00 (PDF=0.1357), time=0.16ms\n",
      "  λ=1.0: peak at t=2.00 (PDF=0.2720), time=0.05ms\n",
      "  λ=2.0: peak at t=1.00 (PDF=0.5468), time=0.05ms\n",
      "\n",
      "Computing moments...\n",
      "  E[T]   = 3.0000 (expected: 3.0000)\n",
      "  E[T²]  = 28.0000\n",
      "  E[T³]  = 17220.0000\n",
      "  E[T⁴]  = 1628322567777048.0000\n",
      "  Mean = 3.0000, Variance = 19.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: building reward compute graph...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Part 2: Direct GraphBuilder Usage (pybind11)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Part 2: Direct GraphBuilder Usage (pybind11)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from ptdalgorithms.ptdalgorithmscpp_pybind import parameterized\n",
    "\n",
    "# Create GraphBuilder\n",
    "builder = parameterized.GraphBuilder(erlang_json)\n",
    "\n",
    "print(f\"GraphBuilder created:\")\n",
    "print(f\"  - param_length: {builder.param_length}\")\n",
    "print(f\"  - state_length: {builder.state_length}\")\n",
    "print(f\"  - vertices_length: {builder.vertices_length}\")\n",
    "print()\n",
    "\n",
    "# Compute PMF for different rate parameters\n",
    "theta_values = [0.5, 1.0, 2.0]\n",
    "times = np.linspace(0.1, 10.0, 100)\n",
    "\n",
    "print(\"Computing PDF for different rate parameters...\")\n",
    "for theta_val in theta_values:\n",
    "    theta = np.array([theta_val])\n",
    "\n",
    "    # Time the computation\n",
    "    start_time = time.time()\n",
    "    pdf = builder.compute_pmf(theta, times, discrete=False, granularity=100)\n",
    "    elapsed = (time.time() - start_time) * 1000\n",
    "\n",
    "    # Find peak\n",
    "    peak_idx = np.argmax(pdf)\n",
    "    peak_time = times[peak_idx]\n",
    "    peak_value = pdf[peak_idx]\n",
    "\n",
    "    print(f\"  λ={theta_val}: peak at t={peak_time:.2f} \"\n",
    "          f\"(PDF={peak_value:.4f}), time={elapsed:.2f}ms\")\n",
    "print()\n",
    "\n",
    "# Compute moments\n",
    "print(\"Computing moments...\")\n",
    "theta = np.array([1.0])\n",
    "moments = builder.compute_moments(theta, nr_moments=4)\n",
    "\n",
    "print(f\"  E[T]   = {moments[0]:.4f} (expected: {3/1.0:.4f})\")\n",
    "print(f\"  E[T²]  = {moments[1]:.4f}\")\n",
    "print(f\"  E[T³]  = {moments[2]:.4f}\")\n",
    "print(f\"  E[T⁴]  = {moments[3]:.4f}\")\n",
    "\n",
    "# Compute mean and variance\n",
    "mean = moments[0]\n",
    "variance = moments[1] - moments[0]**2\n",
    "print(f\"  Mean = {mean:.4f}, Variance = {variance:.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aefc73c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Part 3: JAX FFI Wrappers with pure_callback\n",
      "================================================================================\n",
      "Computing PDF with JAX FFI wrapper...\n",
      "  Times: [1. 2. 3. 4. 5.]\n",
      "  PDF:   [0.1829975  0.27203301 0.22516986 0.14689203 0.08413789]\n",
      "\n",
      "Computing moments with JAX FFI wrapper...\n",
      "  Moments: [3.000e+00 2.800e+01 1.722e+04]\n",
      "\n",
      "Computing PMF + moments together (efficient for SVGD)...\n",
      "  PDF:     [0.1829975  0.27203301 0.22516986 0.14689203 0.08413789]\n",
      "  Moments: [3.000e+00 2.800e+01 1.722e+04]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: building reward compute graph...\n",
      "INFO: building reward compute graph...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Part 3: JAX FFI Wrappers (pure_callback)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Part 3: JAX FFI Wrappers with pure_callback\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from ptdalgorithms.ffi_wrappers import (\n",
    "    compute_pmf_ffi,\n",
    "    compute_moments_ffi,\n",
    "    compute_pmf_and_moments_ffi,\n",
    ")\n",
    "\n",
    "# Convert to JAX arrays\n",
    "theta_jax = jnp.array([1.0])\n",
    "times_jax = jnp.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "\n",
    "print(\"Computing PDF with JAX FFI wrapper...\")\n",
    "pdf_jax = compute_pmf_ffi(erlang_json, theta_jax, times_jax, discrete=False)\n",
    "print(f\"  Times: {times_jax}\")\n",
    "print(f\"  PDF:   {pdf_jax}\")\n",
    "print()\n",
    "\n",
    "print(\"Computing moments with JAX FFI wrapper...\")\n",
    "moments_jax = compute_moments_ffi(erlang_json, theta_jax, nr_moments=3)\n",
    "print(f\"  Moments: {moments_jax}\")\n",
    "print()\n",
    "\n",
    "print(\"Computing PMF + moments together (efficient for SVGD)...\")\n",
    "pdf_combined, moments_combined = compute_pmf_and_moments_ffi(\n",
    "    erlang_json, theta_jax, times_jax, nr_moments=3, discrete=False\n",
    ")\n",
    "print(f\"  PDF:     {pdf_combined}\")\n",
    "print(f\"  Moments: {moments_combined}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0abaf5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Part 4: JIT Compilation\n",
      "================================================================================\n",
      "First call (compilation + execution)...\n",
      "  Time: 10.51ms\n",
      "  Result: [0.1829975  0.27203301 0.22516986 0.14689203 0.08413789]\n",
      "\n",
      "Second call (cached, should be faster)...\n",
      "  Time: 0.26ms\n",
      "  Result: [0.1829975  0.27203301 0.22516986 0.14689203 0.08413789]\n",
      "  Speedup: 40.3x\n",
      "  Results match: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Part 4: JIT Compilation\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Part 4: JIT Compilation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create JIT-compiled version\n",
    "compute_pmf_jit = jax.jit(compute_pmf_ffi, static_argnums=(0, 3, 4))\n",
    "\n",
    "print(\"First call (compilation + execution)...\")\n",
    "start_time = time.time()\n",
    "pdf_jit_1 = compute_pmf_jit(erlang_json, theta_jax, times_jax, False, 100)\n",
    "time_1 = (time.time() - start_time) * 1000\n",
    "print(f\"  Time: {time_1:.2f}ms\")\n",
    "print(f\"  Result: {pdf_jit_1}\")\n",
    "print()\n",
    "\n",
    "print(\"Second call (cached, should be faster)...\")\n",
    "start_time = time.time()\n",
    "pdf_jit_2 = compute_pmf_jit(erlang_json, theta_jax, times_jax, False, 100)\n",
    "time_2 = (time.time() - start_time) * 1000\n",
    "print(f\"  Time: {time_2:.2f}ms\")\n",
    "print(f\"  Result: {pdf_jit_2}\")\n",
    "print(f\"  Speedup: {time_1/time_2:.1f}x\")\n",
    "print(f\"  Results match: {jnp.allclose(pdf_jit_1, pdf_jit_2)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f36a518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Part 5: vmap Batching for Multi-Parameter Inference\n",
      "================================================================================\n",
      "Simulating SVGD with 10 particles...\n",
      "Parameter values (λ): [0.5        0.66666667 0.83333333 1.         1.16666667]... 2.0\n",
      "\n",
      "Computing PDF for all particles with vmap...\n",
      "  Batch computation time: 40.03ms\n",
      "  Result shape: (10, 5) (particles × time_points)\n",
      "  First particle PDF: [0.03728895 0.09173714 0.12566735 0.13567475 0.12861187]\n",
      "  Last particle PDF:  [0.54682782 0.29452026 0.08832668 0.020877   0.00433261]\n",
      "\n",
      "Batch statistics across 10 particles:\n",
      "  Mean PDF: [0.2840928  0.26640083 0.16695124 0.09897579 0.06013171]\n",
      "  Std PDF:  [0.17021455 0.08001794 0.04555659 0.05238023 0.04812983]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Part 5: vmap Batching (Multi-Parameter Inference)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Part 5: vmap Batching for Multi-Parameter Inference\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Simulate SVGD particles (multiple rate parameters)\n",
    "num_particles = 10\n",
    "theta_batch = jnp.linspace(0.5, 2.0, num_particles).reshape(-1, 1)\n",
    "\n",
    "print(f\"Simulating SVGD with {num_particles} particles...\")\n",
    "print(f\"Parameter values (λ): {theta_batch.flatten()[:5]}... {theta_batch.flatten()[-1]}\")\n",
    "print()\n",
    "\n",
    "# Define function to vmap over\n",
    "def compute_pdf_for_particle(theta):\n",
    "    return compute_pmf_ffi(erlang_json, theta, times_jax, discrete=False)\n",
    "\n",
    "# Apply vmap\n",
    "print(\"Computing PDF for all particles with vmap...\")\n",
    "vmap_compute_pdf = jax.vmap(compute_pdf_for_particle)\n",
    "\n",
    "start_time = time.time()\n",
    "pdf_batch = vmap_compute_pdf(theta_batch)\n",
    "elapsed = (time.time() - start_time) * 1000\n",
    "\n",
    "print(f\"  Batch computation time: {elapsed:.2f}ms\")\n",
    "print(f\"  Result shape: {pdf_batch.shape} (particles × time_points)\")\n",
    "print(f\"  First particle PDF: {pdf_batch[0]}\")\n",
    "print(f\"  Last particle PDF:  {pdf_batch[-1]}\")\n",
    "print()\n",
    "\n",
    "# Visualize batch statistics\n",
    "pdf_mean = jnp.mean(pdf_batch, axis=0)\n",
    "pdf_std = jnp.std(pdf_batch, axis=0)\n",
    "print(f\"Batch statistics across {num_particles} particles:\")\n",
    "print(f\"  Mean PDF: {pdf_mean}\")\n",
    "print(f\"  Std PDF:  {pdf_std}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1661bfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Part 6: SVGD-like Workflow with Moment Regularization\n",
      "================================================================================\n",
      "Target moments: [ 3. 12. 60.]\n",
      "\n",
      "Evaluating SVGD objective for different parameters...\n",
      "  λ=0.5: objective=-31093044761347.1016\n",
      "  λ=1.0: objective=-29446594.4844\n",
      "  λ=1.5: objective=-5705.7982\n",
      "  λ=2.0: objective=-207.4957\n",
      "\n",
      "Batch evaluation for 10 particles with vmap...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: building reward compute graph...\n",
      "INFO: building reward compute graph...\n",
      "INFO: building reward compute graph...\n",
      "INFO: building reward compute graph...\n",
      "INFO: building reward compute graph...\n",
      "INFO: building reward compute graph...\n",
      "INFO: building reward compute graph...\n",
      "INFO: building reward compute graph...\n",
      "INFO: building reward compute graph...\n",
      "INFO: building reward compute graph...\n",
      "INFO: building reward compute graph...\n",
      "INFO: building reward compute graph...\n",
      "INFO: building reward compute graph...\n",
      "INFO: building reward compute graph...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Time: 139.49ms\n",
      "  Objectives: [-3.10930448e+13 -9.85913909e+10 -1.13553976e+09 -2.94465945e+07\n",
      " -1.31486092e+06 -8.27684529e+04 -5.70579823e+03 -2.09624190e+02\n",
      " -5.75172066e+01 -2.07495711e+02]\n",
      "  Best particle: λ=1.8333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Part 6: SVGD-like Workflow (PMF + Moments)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Part 6: SVGD-like Workflow with Moment Regularization\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Target moments (what we want to match)\n",
    "target_moments = jnp.array([3.0, 12.0, 60.0])  # E[T]=3, E[T²]=12, E[T³]=60\n",
    "print(f\"Target moments: {target_moments}\")\n",
    "print()\n",
    "\n",
    "def svgd_objective(theta, observations, target_moments_local):\n",
    "    \"\"\"\n",
    "    SVGD objective: log-likelihood + moment matching penalty.\n",
    "\n",
    "    This demonstrates the efficient combined computation.\n",
    "    \"\"\"\n",
    "    # Compute PMF and moments in one call (efficient!)\n",
    "    pdf, moments = compute_pmf_and_moments_ffi(\n",
    "        erlang_json, theta, observations, nr_moments=3, discrete=False\n",
    "    )\n",
    "\n",
    "    # Log-likelihood (avoid log(0))\n",
    "    log_likelihood = jnp.sum(jnp.log(pdf + 1e-10))\n",
    "\n",
    "    # Moment matching penalty\n",
    "    moment_penalty = jnp.sum((moments - target_moments_local)**2)\n",
    "\n",
    "    # Combined objective\n",
    "    return log_likelihood - 0.1 * moment_penalty\n",
    "\n",
    "# Evaluate objective for different parameters\n",
    "print(\"Evaluating SVGD objective for different parameters...\")\n",
    "observations = jnp.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "\n",
    "for theta_val in [0.5, 1.0, 1.5, 2.0]:\n",
    "    theta_test = jnp.array([theta_val])\n",
    "    objective_val = svgd_objective(theta_test, observations, target_moments)\n",
    "    print(f\"  λ={theta_val}: objective={objective_val:.4f}\")\n",
    "print()\n",
    "\n",
    "# Batch evaluation with vmap\n",
    "print(f\"Batch evaluation for {num_particles} particles with vmap...\")\n",
    "def eval_objective(theta):\n",
    "    return svgd_objective(theta, observations, target_moments)\n",
    "\n",
    "vmap_eval = jax.vmap(eval_objective)\n",
    "start_time = time.time()\n",
    "objectives_batch = vmap_eval(theta_batch)\n",
    "elapsed = (time.time() - start_time) * 1000\n",
    "\n",
    "print(f\"  Time: {elapsed:.2f}ms\")\n",
    "print(f\"  Objectives: {objectives_batch}\")\n",
    "print(f\"  Best particle: λ={theta_batch[jnp.argmax(objectives_batch)][0]:.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1619af7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Part 7: Performance Comparison\n",
      "================================================================================\n",
      "Running 100 iterations...\n",
      "\n",
      "1. Direct pybind11 (numpy):\n",
      "   Total: 3.53ms, Per iteration: 0.035ms\n",
      "2. JAX FFI wrapper (no JIT):\n",
      "   Total: 684.91ms, Per iteration: 6.849ms\n",
      "3. JAX FFI wrapper (with JIT):\n",
      "   Total: 12.23ms, Per iteration: 0.122ms\n",
      "4. Batch computation (vmap, 10 particles):\n",
      "   Total: 346.21ms, Per particle: 3.462ms\n",
      "\n",
      "Summary:\n",
      "  Direct pybind11:    0.035ms per call (baseline)\n",
      "  FFI no JIT:         6.849ms per call (0.01x)\n",
      "  FFI with JIT:       0.122ms per call (0.29x)\n",
      "  Batch (vmap):       3.462ms per call (0.10x)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Part 7: Performance Comparison\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Part 7: Performance Comparison\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Compare different approaches\n",
    "num_iterations = 100\n",
    "theta_test = np.array([1.0])\n",
    "times_test = np.linspace(0.1, 5.0, 50)\n",
    "\n",
    "print(f\"Running {num_iterations} iterations...\")\n",
    "print()\n",
    "\n",
    "# 1. Direct pybind11 (numpy)\n",
    "print(\"1. Direct pybind11 (numpy):\")\n",
    "start_time = time.time()\n",
    "for _ in range(num_iterations):\n",
    "    pdf_direct = builder.compute_pmf(theta_test, times_test, discrete=False)\n",
    "time_direct = (time.time() - start_time) * 1000\n",
    "print(f\"   Total: {time_direct:.2f}ms, Per iteration: {time_direct/num_iterations:.3f}ms\")\n",
    "\n",
    "# 2. JAX FFI wrapper (pure_callback, no JIT)\n",
    "print(\"2. JAX FFI wrapper (no JIT):\")\n",
    "theta_jax_test = jnp.array([1.0])\n",
    "times_jax_test = jnp.array(times_test)\n",
    "start_time = time.time()\n",
    "for _ in range(num_iterations):\n",
    "    pdf_ffi = compute_pmf_ffi(erlang_json, theta_jax_test, times_jax_test, discrete=False)\n",
    "time_ffi = (time.time() - start_time) * 1000\n",
    "print(f\"   Total: {time_ffi:.2f}ms, Per iteration: {time_ffi/num_iterations:.3f}ms\")\n",
    "\n",
    "# 3. JAX FFI wrapper (with JIT)\n",
    "print(\"3. JAX FFI wrapper (with JIT):\")\n",
    "compute_pmf_jit_test = jax.jit(compute_pmf_ffi, static_argnums=(0, 3, 4))\n",
    "# Warm-up\n",
    "_ = compute_pmf_jit_test(erlang_json, theta_jax_test, times_jax_test, False, 100)\n",
    "start_time = time.time()\n",
    "for _ in range(num_iterations):\n",
    "    pdf_jit_test = compute_pmf_jit_test(erlang_json, theta_jax_test, times_jax_test, False, 100)\n",
    "time_jit = (time.time() - start_time) * 1000\n",
    "print(f\"   Total: {time_jit:.2f}ms, Per iteration: {time_jit/num_iterations:.3f}ms\")\n",
    "\n",
    "# 4. Batch computation (vmap)\n",
    "print(f\"4. Batch computation (vmap, {num_particles} particles):\")\n",
    "def batch_compute(theta_single):\n",
    "    return compute_pmf_ffi(erlang_json, theta_single, times_jax_test, discrete=False)\n",
    "vmap_batch_compute = jax.vmap(batch_compute)\n",
    "theta_batch_test = jnp.linspace(0.5, 2.0, num_particles).reshape(-1, 1)\n",
    "# Warm-up\n",
    "_ = vmap_batch_compute(theta_batch_test)\n",
    "start_time = time.time()\n",
    "for _ in range(num_iterations // num_particles):  # Fair comparison\n",
    "    pdf_batch_test = vmap_batch_compute(theta_batch_test)\n",
    "time_batch = (time.time() - start_time) * 1000\n",
    "time_batch_per_particle = time_batch / (num_iterations // num_particles) / num_particles\n",
    "print(f\"   Total: {time_batch:.2f}ms, Per particle: {time_batch_per_particle:.3f}ms\")\n",
    "\n",
    "print()\n",
    "print(\"Summary:\")\n",
    "print(f\"  Direct pybind11:    {time_direct/num_iterations:.3f}ms per call (baseline)\")\n",
    "print(f\"  FFI no JIT:         {time_ffi/num_iterations:.3f}ms per call ({time_direct/time_ffi:.2f}x)\")\n",
    "print(f\"  FFI with JIT:       {time_jit/num_iterations:.3f}ms per call ({time_direct/time_jit:.2f}x)\")\n",
    "print(f\"  Batch (vmap):       {time_batch_per_particle:.3f}ms per call ({time_direct/(time_batch_per_particle*num_iterations//num_particles):.2f}x)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "053dffaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Part 8: Multi-CPU and Multi-Device Parallelization\n",
      "================================================================================\n",
      "Available CPU devices: 1\n",
      "Devices: [CpuDevice(id=0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Part 8: Multi-CPU and Multi-Device Parallelization\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Part 8: Multi-CPU and Multi-Device Parallelization\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check available devices\n",
    "cpu_devices = jax.devices('cpu')\n",
    "num_cpus = len(cpu_devices)\n",
    "print(f\"Available CPU devices: {num_cpus}\")\n",
    "print(f\"Devices: {cpu_devices[:min(4, num_cpus)]}\")  # Show first 4\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a21472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 8.1: pmap (Parallel Map) vs vmap (Sequential) ---\n",
      "\n",
      "Testing with 4 particles across 1 CPU devices\n",
      "  Particles per device: 4\n",
      "\n",
      "1. Sequential vmap (single device):\n",
      "   Time: 27.32ms\n",
      "   Result shape: (4, 5)\n",
      "\n",
      "2. Parallel pmap (1 devices):\n",
      "   Input shape for pmap: (1, 4, 1)\n",
      "   Time: 0.65ms\n",
      "   Result shape: (1, 4, 5)\n",
      "   Speedup: 41.77x\n",
      "\n",
      "   Max difference between vmap and pmap: 0.00e+00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 8.1: pmap - Parallel Map Across Devices\n",
    "# ============================================================================\n",
    "\n",
    "print(\"--- 8.1: pmap (Parallel Map) vs vmap (Sequential) ---\")\n",
    "print()\n",
    "\n",
    "# Create a batch of particles that divides evenly across available CPUs\n",
    "# For pmap, batch size should be divisible by number of devices\n",
    "num_pmap_particles = num_cpus * 4  # 4 particles per CPU\n",
    "theta_pmap_batch = jnp.linspace(0.5, 2.0, num_pmap_particles).reshape(-1, 1)\n",
    "times_pmap = jnp.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "\n",
    "print(f\"Testing with {num_pmap_particles} particles across {num_cpus} CPU devices\")\n",
    "print(f\"  Particles per device: {num_pmap_particles // num_cpus}\")\n",
    "print()\n",
    "\n",
    "# Define computation function for a single particle\n",
    "def compute_pdf_single(theta):\n",
    "    \"\"\"Compute PDF for a single parameter value.\"\"\"\n",
    "    return compute_pmf_ffi(erlang_json, theta, times_pmap, discrete=False)\n",
    "\n",
    "# 1. Sequential vmap (runs on single device)\n",
    "print(\"1. Sequential vmap (single device):\")\n",
    "vmap_compute = jax.vmap(compute_pdf_single)\n",
    "\n",
    "# Warm-up\n",
    "_ = vmap_compute(theta_pmap_batch)\n",
    "\n",
    "# Time it\n",
    "start_time = time.time()\n",
    "pdf_vmap = vmap_compute(theta_pmap_batch)\n",
    "time_vmap = (time.time() - start_time) * 1000\n",
    "print(f\"   Time: {time_vmap:.2f}ms\")\n",
    "print(f\"   Result shape: {pdf_vmap.shape}\")\n",
    "print()\n",
    "\n",
    "# 2. Parallel pmap (distributes across devices)\n",
    "print(f\"2. Parallel pmap ({num_cpus} devices):\")\n",
    "\n",
    "# Reshape batch to split across devices: (n_devices, particles_per_device, n_params)\n",
    "theta_pmap_split = theta_pmap_batch.reshape(num_cpus, num_pmap_particles // num_cpus, 1)\n",
    "print(f\"   Input shape for pmap: {theta_pmap_split.shape}\")\n",
    "\n",
    "# Define pmap function that maps over first axis (devices)\n",
    "pmap_compute = jax.pmap(vmap_compute, in_axes=0)\n",
    "\n",
    "# Warm-up\n",
    "_ = pmap_compute(theta_pmap_split)\n",
    "\n",
    "# Time it\n",
    "start_time = time.time()\n",
    "pdf_pmap = pmap_compute(theta_pmap_split)\n",
    "time_pmap = (time.time() - start_time) * 1000\n",
    "print(f\"   Time: {time_pmap:.2f}ms\")\n",
    "print(f\"   Result shape: {pdf_pmap.shape}\")\n",
    "print(f\"   Speedup: {time_vmap / time_pmap:.2f}x\")\n",
    "print()\n",
    "\n",
    "# Verify results are the same\n",
    "pdf_pmap_flat = pdf_pmap.reshape(num_pmap_particles, -1)\n",
    "difference = jnp.max(jnp.abs(pdf_vmap - pdf_pmap_flat))\n",
    "print(f\"   Max difference between vmap and pmap: {difference:.2e}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b900e488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 8.2: Parallel SVGD Objective Evaluation ---\n",
      "\n",
      "1. Sequential evaluation (vmap, 4 particles):\n",
      "   Time: 153.14ms\n",
      "\n",
      "2. Parallel evaluation (pmap, 1 devices):\n",
      "   Time: 39.85ms\n",
      "   Speedup: 3.84x\n",
      "\n",
      "   Best particle: λ=2.0000\n",
      "   Best objective: -207.50\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: building reward compute graph...\n",
      "INFO: building reward compute graph...\n",
      "INFO: building reward compute graph...\n",
      "INFO: building reward compute graph...\n",
      "INFO: building reward compute graph...\n",
      "INFO: building reward compute graph...\n",
      "INFO: building reward compute graph...\n",
      "INFO: building reward compute graph...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 8.2: Parallel SVGD with pmap\n",
    "# ============================================================================\n",
    "\n",
    "print(\"--- 8.2: Parallel SVGD Objective Evaluation ---\")\n",
    "print()\n",
    "\n",
    "# Use pmap to parallelize SVGD objective computation across particles\n",
    "target_moments_parallel = jnp.array([3.0, 12.0, 60.0])\n",
    "observations_parallel = jnp.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "\n",
    "def svgd_objective_single(theta):\n",
    "    \"\"\"SVGD objective for a single particle.\"\"\"\n",
    "    pdf, moments = compute_pmf_and_moments_ffi(\n",
    "        erlang_json, theta, observations_parallel,\n",
    "        nr_moments=3, discrete=False\n",
    "    )\n",
    "    log_likelihood = jnp.sum(jnp.log(pdf + 1e-10))\n",
    "    moment_penalty = jnp.sum((moments - target_moments_parallel)**2)\n",
    "    return log_likelihood - 0.1 * moment_penalty\n",
    "\n",
    "# Sequential vmap evaluation\n",
    "print(f\"1. Sequential evaluation (vmap, {num_pmap_particles} particles):\")\n",
    "vmap_objective = jax.vmap(svgd_objective_single)\n",
    "start_time = time.time()\n",
    "objectives_vmap = vmap_objective(theta_pmap_batch)\n",
    "time_vmap_obj = (time.time() - start_time) * 1000\n",
    "print(f\"   Time: {time_vmap_obj:.2f}ms\")\n",
    "print()\n",
    "\n",
    "# Parallel pmap evaluation\n",
    "print(f\"2. Parallel evaluation (pmap, {num_cpus} devices):\")\n",
    "pmap_objective = jax.pmap(vmap_objective, in_axes=0)\n",
    "start_time = time.time()\n",
    "objectives_pmap = pmap_objective(theta_pmap_split)\n",
    "time_pmap_obj = (time.time() - start_time) * 1000\n",
    "print(f\"   Time: {time_pmap_obj:.2f}ms\")\n",
    "print(f\"   Speedup: {time_vmap_obj / time_pmap_obj:.2f}x\")\n",
    "print()\n",
    "\n",
    "# Show best particle\n",
    "objectives_pmap_flat = objectives_pmap.reshape(-1)\n",
    "best_idx = jnp.argmax(objectives_pmap_flat)\n",
    "print(f\"   Best particle: λ={theta_pmap_batch[best_idx][0]:.4f}\")\n",
    "print(f\"   Best objective: {objectives_pmap_flat[best_idx]:.2f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4916f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 8.3: Device Load Balancing ---\n",
      "\n",
      "Work distribution across 1 CPUs:\n",
      "  Total particles: 4\n",
      "  Particles per device: 4\n",
      "  Input shape to pmap: (1, 4, 1)\n",
      "    - Axis 0 (devices): 1\n",
      "    - Axis 1 (particles per device): 4\n",
      "    - Axis 2 (parameters): 1\n",
      "\n",
      "Performance Summary:\n",
      "  PDF computation:\n",
      "    - vmap (sequential): 27.32ms\n",
      "    - pmap (parallel):   0.65ms (41.77x speedup)\n",
      "  SVGD objective:\n",
      "    - vmap (sequential): 153.14ms\n",
      "    - pmap (parallel):   39.85ms (3.84x speedup)\n",
      "\n",
      "Key Insights:\n",
      "  • pmap distributes computation across 1 CPU cores\n",
      "  • Sequential vmap processes all particles on single device\n",
      "  • Parallel pmap provides ~22.8x average speedup\n",
      "  • Effective for SVGD with many particles (>2)\n",
      "  • Batch size should be divisible by num_devices for best balance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 8.3: Multi-Device Statistics and Load Balancing\n",
    "# ============================================================================\n",
    "\n",
    "print(\"--- 8.3: Device Load Balancing ---\")\n",
    "print()\n",
    "\n",
    "# Demonstrate how pmap distributes work across devices\n",
    "print(f\"Work distribution across {num_cpus} CPUs:\")\n",
    "print(f\"  Total particles: {num_pmap_particles}\")\n",
    "print(f\"  Particles per device: {num_pmap_particles // num_cpus}\")\n",
    "print(f\"  Input shape to pmap: {theta_pmap_split.shape}\")\n",
    "print(f\"    - Axis 0 (devices): {theta_pmap_split.shape[0]}\")\n",
    "print(f\"    - Axis 1 (particles per device): {theta_pmap_split.shape[1]}\")\n",
    "print(f\"    - Axis 2 (parameters): {theta_pmap_split.shape[2]}\")\n",
    "print()\n",
    "\n",
    "# Performance summary\n",
    "print(\"Performance Summary:\")\n",
    "print(f\"  PDF computation:\")\n",
    "print(f\"    - vmap (sequential): {time_vmap:.2f}ms\")\n",
    "print(f\"    - pmap (parallel):   {time_pmap:.2f}ms ({time_vmap/time_pmap:.2f}x speedup)\")\n",
    "print(f\"  SVGD objective:\")\n",
    "print(f\"    - vmap (sequential): {time_vmap_obj:.2f}ms\")\n",
    "print(f\"    - pmap (parallel):   {time_pmap_obj:.2f}ms ({time_vmap_obj/time_pmap_obj:.2f}x speedup)\")\n",
    "print()\n",
    "\n",
    "print(\"Key Insights:\")\n",
    "print(f\"  • pmap distributes computation across {num_cpus} CPU cores\")\n",
    "print(f\"  • Sequential vmap processes all particles on single device\")\n",
    "print(f\"  • Parallel pmap provides ~{(time_vmap/time_pmap + time_vmap_obj/time_pmap_obj)/2:.1f}x average speedup\")\n",
    "print(f\"  • Effective for SVGD with many particles (>{num_cpus*2})\")\n",
    "print(f\"  • Batch size should be divisible by num_devices for best balance\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbc31a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Part 9: Summary & Key Takeaways\n",
      "================================================================================\n",
      "\n",
      "GraphBuilder successfully separates structure from parameters\n",
      "JAX FFI wrappers enable JIT compilation and vmap batching\n",
      "Combined PMF + moments computation is efficient for SVGD\n",
      "vmap enables batch processing of multiple parameter values\n",
      "pmap enables parallel processing across multiple CPU devices\n",
      "JIT compilation provides speedup after initial compilation\n",
      "\n",
      "Key Features:\n",
      "  • Parameterized phase-type distributions with theta parameters\n",
      "  • JAX pure_callback integration for JIT compatibility\n",
      "  • Sequential vmap batching for multi-parameter inference\n",
      "  • Parallel pmap execution across multiple CPU devices\n",
      "  • Combined PMF+moments for moment-based regularization\n",
      "  • Thread-safe via JAX's automatic GIL management\n",
      "\n",
      "Ready for:\n",
      "  • SVGD inference with moment-based regularization\n",
      "  • Multi-chain MCMC with parallel likelihood evaluation\n",
      "  • Multi-device parallel inference across CPU cores\n",
      "  • Parameter optimization with gradient-free methods\n",
      "  • Batch inference over multiple parameter sets\n",
      "\n",
      "Next steps:\n",
      "  • Implement custom VJP rules for gradient-based optimization\n",
      "  • Expose native XLA FFI handlers for better performance\n",
      "  • Add GPU support for large-scale inference\n",
      "\n",
      "================================================================================\n",
      "Showcase Complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Part 9: Summary & Key Takeaways\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Part 9: Summary & Key Takeaways\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"GraphBuilder successfully separates structure from parameters\")\n",
    "print(\"JAX FFI wrappers enable JIT compilation and vmap batching\")\n",
    "print(\"Combined PMF + moments computation is efficient for SVGD\")\n",
    "print(\"vmap enables batch processing of multiple parameter values\")\n",
    "print(\"pmap enables parallel processing across multiple CPU devices\")\n",
    "print(\"JIT compilation provides speedup after initial compilation\")\n",
    "print()\n",
    "print(\"Key Features:\")\n",
    "print(\"  • Parameterized phase-type distributions with theta parameters\")\n",
    "print(\"  • JAX pure_callback integration for JIT compatibility\")\n",
    "print(\"  • Sequential vmap batching for multi-parameter inference\")\n",
    "print(\"  • Parallel pmap execution across multiple CPU devices\")\n",
    "print(\"  • Combined PMF+moments for moment-based regularization\")\n",
    "print(\"  • Thread-safe via JAX's automatic GIL management\")\n",
    "print()\n",
    "print(\"Ready for:\")\n",
    "print(\"  • SVGD inference with moment-based regularization\")\n",
    "print(\"  • Multi-chain MCMC with parallel likelihood evaluation\")\n",
    "print(\"  • Multi-device parallel inference across CPU cores\")\n",
    "print(\"  • Parameter optimization with gradient-free methods\")\n",
    "print(\"  • Batch inference over multiple parameter sets\")\n",
    "print()\n",
    "print(\"Next steps:\")\n",
    "print(\"  • Implement custom VJP rules for gradient-based optimization\")\n",
    "print(\"  • Expose native XLA FFI handlers for better performance\")\n",
    "print(\"  • Add GPU support for large-scale inference\")\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"Showcase Complete!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2d5ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
