{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: SVGD Inference\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides complete examples of Stein Variational Gradient Descent (SVGD) inference with phase-type distributions.\n",
    "\n",
    "## Topics Covered\n",
    "\n",
    "1. **Basic SVGD** - Single CPU, continuous models\n",
    "2. **Discrete Models** - SVGD with discrete phase-type distributions\n",
    "3. **Multi-CPU Parallelization** - Automatic resource detection and usage\n",
    "4. **Distributed Computing** - SLURM multi-node examples\n",
    "5. **Advanced Options** - Batching, sharding, regularization\n",
    "6. **Different Approaches** - Multiple ways to accomplish the same task\n",
    "7. **Troubleshooting** - Common issues and solutions\n",
    "\n",
    "## What is SVGD?\n",
    "\n",
    "SVGD is a Bayesian inference algorithm that:\n",
    "- Approximates posterior distributions p(θ | data)\n",
    "- Uses a set of particles that evolve to represent the posterior\n",
    "- Supports automatic differentiation via JAX\n",
    "- Scales to multiple CPUs and distributed systems\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "# Install with JAX support\n",
    "pip install ptdalgorithms[jax]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# JAX imports\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# PtDAlgorithms imports\n",
    "import ptdalgorithms as pta\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "print(f\"PtDAlgorithms version: {pta.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Basic SVGD with Continuous Models\n",
    "\n",
    "We'll start with a simple coalescent model and perform SVGD inference on a single CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Build a Parameterized Coalescent Model\n",
    "\n",
    "The coalescent model describes genealogies. We parameterize the coalescence rate with θ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coalescent_callback(state, nr_samples=3):\n",
    "    \"\"\"\n",
    "    Parameterized coalescent model.\n",
    "    \n",
    "    Returns transitions with parameterized rates.\n",
    "    The actual rate = edge_state @ theta\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    state : array\n",
    "        Current state [n_lineages]\n",
    "    nr_samples : int\n",
    "        Number of initial samples\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of (next_state, weight, edge_state) tuples\n",
    "    \"\"\"\n",
    "    if len(state) == 0:\n",
    "        # Initial state: all samples are separate lineages\n",
    "        initial = np.array([nr_samples])\n",
    "        # Edge state [1.0] means rate = 1.0 * theta\n",
    "        return [(initial, 1.0, [1.0])]\n",
    "    \n",
    "    if state[0] > 1:\n",
    "        # Coalescence: n lineages → n-1 lineages\n",
    "        n = state[0]\n",
    "        new_state = np.array([n - 1])\n",
    "        \n",
    "        # Coalescence rate for n lineages: n*(n-1)/2\n",
    "        # Edge state = [n*(n-1)/2], so actual_rate = [n*(n-1)/2] @ theta\n",
    "        rate = n * (n - 1) / 2\n",
    "        return [(new_state, 0.0, [rate])]\n",
    "    \n",
    "    # Absorbing state (MRCA reached)\n",
    "    return []\n",
    "\n",
    "# Build the graph\n",
    "nr_samples = 4\n",
    "graph = pta.Graph(callback=coalescent_callback, parameterized=True, nr_samples=nr_samples)\n",
    "\n",
    "print(f\"Built coalescent graph:\")\n",
    "print(f\"  Samples: {nr_samples}\")\n",
    "print(f\"  States: {graph.vertices_length()}\")\n",
    "print(f\"  Parameters: 1 (θ controls all coalescence rates)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Convert to JAX-Compatible Model\n",
    "\n",
    "The model must have signature: `model(theta, times) -> pdf_values`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert graph to JAX-compatible function\n",
    "# discrete=False for continuous time (PDF)\n",
    "model_continuous = pta.Graph.pmf_from_graph(graph, discrete=False)\n",
    "\n",
    "# Test the model\n",
    "test_theta = jnp.array([1.0])\n",
    "test_times = jnp.array([0.5, 1.0, 1.5])\n",
    "test_pdf = model_continuous(test_theta, test_times)\n",
    "\n",
    "print(f\"Model signature: model(theta, times) -> pdf_values\")\n",
    "print(f\"Test input:  theta={test_theta}, times={test_times}\")\n",
    "print(f\"Test output: pdf={test_pdf}\")\n",
    "print(f\"\\nModel supports JAX transformations: jit, grad, vmap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Generate Synthetic Observed Data\n",
    "\n",
    "For demonstration, we'll create synthetic data from a known parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True parameter (we'll try to recover this)\n",
    "true_theta = jnp.array([0.8])\n",
    "\n",
    "# Evaluation times\n",
    "observed_times = jnp.linspace(0.1, 4.0, 20)\n",
    "\n",
    "# Generate \"observed\" PDF values\n",
    "true_pdf = model_continuous(true_theta, observed_times)\n",
    "\n",
    "# Add small noise to simulate real observations\n",
    "np.random.seed(42)\n",
    "noise_level = 0.05\n",
    "noise = np.random.normal(0, noise_level * float(jnp.max(true_pdf)), size=true_pdf.shape)\n",
    "observed_pdf = jnp.maximum(true_pdf + noise, 1e-10)  # Keep positive\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(observed_times, true_pdf, 'b-', linewidth=2, label='True PDF', alpha=0.7)\n",
    "plt.scatter(observed_times, observed_pdf, color='red', s=50, \n",
    "           label='Observed (noisy)', alpha=0.6, zorder=5)\n",
    "plt.xlabel('Time', fontsize=12)\n",
    "plt.ylabel('PDF', fontsize=12)\n",
    "plt.title(f'Synthetic Data (True θ = {true_theta[0]:.2f})', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Generated {len(observed_pdf)} observations\")\n",
    "print(f\"True θ = {true_theta[0]:.3f}\")\n",
    "print(f\"Noise level = {noise_level * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Method 1: Using Graph.svgd() Class Method (Recommended)\n",
    "\n",
    "The simplest way to run SVGD inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SVGD using class method\n",
    "results = pta.Graph.svgd(\n",
    "    model=model_continuous,\n",
    "    observed_data=observed_pdf,\n",
    "    theta_dim=1,              # One parameter to infer\n",
    "    n_particles=30,           # Number of posterior samples\n",
    "    n_iterations=500,         # Optimization steps\n",
    "    learning_rate=0.01,       # Step size\n",
    "    kernel='rbf_median',      # RBF kernel with median heuristic\n",
    "    return_history=True,      # Save particle trajectories\n",
    "    seed=42,                  # For reproducibility\n",
    "    verbose=True              # Print progress\n",
    ")\n",
    "\n",
    "# Extract results\n",
    "posterior_mean = results['theta_mean']\n",
    "posterior_std = results['theta_std']\n",
    "particles = results['particles']\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SVGD Results (Method 1: Class Method)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"True θ:            {true_theta[0]:.4f}\")\n",
    "print(f\"Posterior mean:    {posterior_mean[0]:.4f}\")\n",
    "print(f\"Posterior std:     {posterior_std[0]:.4f}\")\n",
    "print(f\"Error:             {abs(posterior_mean[0] - true_theta[0]):.4f}\")\n",
    "print(f\"95% CI:            [{posterior_mean[0] - 1.96*posterior_std[0]:.4f}, \"\n",
    "      f\"{posterior_mean[0] + 1.96*posterior_std[0]:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Method 2: Using SVGD Class (Object-Oriented)\n",
    "\n",
    "For more control and access to diagnostic methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptdalgorithms import SVGD\n",
    "\n",
    "# Create SVGD object\n",
    "svgd = SVGD(\n",
    "    model=model_continuous,\n",
    "    observed_data=observed_pdf,\n",
    "    theta_dim=1,\n",
    "    n_particles=30,\n",
    "    n_iterations=500,\n",
    "    learning_rate=0.01,\n",
    "    kernel='rbf_median',\n",
    "    seed=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run inference\n",
    "svgd.fit(return_history=True)\n",
    "\n",
    "# Access results as attributes\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SVGD Results (Method 2: SVGD Class)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"True θ:            {true_theta[0]:.4f}\")\n",
    "print(f\"Posterior mean:    {svgd.theta_mean[0]:.4f}\")\n",
    "print(f\"Posterior std:     {svgd.theta_std[0]:.4f}\")\n",
    "print(f\"Error:             {abs(svgd.theta_mean[0] - true_theta[0]):.4f}\")\n",
    "\n",
    "# Use built-in summary method\n",
    "print()\n",
    "svgd.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Visualize Results\n",
    "\n",
    "The SVGD class provides built-in plotting methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot posterior distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Posterior histogram\n",
    "ax = axes[0]\n",
    "ax.hist(svgd.particles[:, 0], bins=20, density=True, alpha=0.7, \n",
    "        color='blue', edgecolor='black')\n",
    "ax.axvline(true_theta[0], color='red', linestyle='--', linewidth=2, \n",
    "          label=f'True θ = {true_theta[0]:.3f}')\n",
    "ax.axvline(svgd.theta_mean[0], color='green', linestyle='-', linewidth=2,\n",
    "          label=f'Posterior mean = {svgd.theta_mean[0]:.3f}')\n",
    "ax.set_xlabel('θ', fontsize=11)\n",
    "ax.set_ylabel('Density', fontsize=11)\n",
    "ax.set_title('Posterior Distribution', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Convergence trace\n",
    "ax = axes[1]\n",
    "if svgd.history is not None:\n",
    "    history = jnp.stack(svgd.history)\n",
    "    # Plot mean trajectory\n",
    "    mean_traj = jnp.mean(history[:, :, 0], axis=1)\n",
    "    ax.plot(mean_traj, linewidth=2, color='blue', label='Particle mean')\n",
    "    ax.axhline(true_theta[0], color='red', linestyle='--', linewidth=2, \n",
    "              label='True θ')\n",
    "    ax.set_xlabel('Iteration', fontsize=11)\n",
    "    ax.set_ylabel('θ', fontsize=11)\n",
    "    ax.set_title('Convergence Trace', fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "# Posterior predictive\n",
    "ax = axes[2]\n",
    "# Sample 20 particles for visualization\n",
    "n_samples = 20\n",
    "sample_indices = np.random.choice(len(svgd.particles), n_samples, replace=False)\n",
    "for idx in sample_indices:\n",
    "    theta_sample = jnp.array([svgd.particles[idx, 0]])\n",
    "    pred_pdf = model_continuous(theta_sample, observed_times)\n",
    "    ax.plot(observed_times, pred_pdf, 'b-', alpha=0.2, linewidth=1)\n",
    "\n",
    "# True and observed\n",
    "ax.plot(observed_times, true_pdf, 'r-', linewidth=2.5, \n",
    "       label='True PDF', alpha=0.8)\n",
    "ax.scatter(observed_times, observed_pdf, color='black', s=40,\n",
    "          label='Observed', alpha=0.6, zorder=5)\n",
    "ax.set_xlabel('Time', fontsize=11)\n",
    "ax.set_ylabel('PDF', fontsize=11)\n",
    "ax.set_title('Posterior Predictive', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: SVGD with Discrete Models\n",
    "\n",
    "Discrete phase-type distributions (DPH) model the number of jumps/events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Build and Discretize the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with the same continuous coalescent model\n",
    "graph_continuous = pta.Graph(callback=coalescent_callback, parameterized=True, nr_samples=4)\n",
    "\n",
    "# Discretize: converts continuous → discrete phase-type\n",
    "graph_discrete, rewards = graph_continuous.discretize(reward_rate=0.1)\n",
    "graph_discrete.normalize()\n",
    "\n",
    "print(f\"Discretized model:\")\n",
    "print(f\"  Original states: {graph_continuous.vertices_length()}\")\n",
    "print(f\"  Discrete states: {graph_discrete.vertices_length()}\")\n",
    "print(f\"  Reward matrix shape: {rewards.shape}\")\n",
    "\n",
    "# Create JAX-compatible discrete model\n",
    "# discrete=True for discrete phase-type (PMF over jump counts)\n",
    "model_discrete = pta.Graph.pmf_from_graph(graph_discrete, discrete=True)\n",
    "\n",
    "# Test\n",
    "test_jumps = jnp.array([0, 1, 2, 5, 10], dtype=jnp.int32)\n",
    "test_pmf = model_discrete(test_theta, test_jumps)\n",
    "print(f\"\\nTest: jumps={test_jumps}\")\n",
    "print(f\"      PMF={test_pmf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Generate Discrete Observed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True parameter\n",
    "true_theta_discrete = jnp.array([0.8])\n",
    "\n",
    "# Observation jumps (integers)\n",
    "observed_jumps = jnp.arange(0, 30, dtype=jnp.int32)\n",
    "\n",
    "# Generate observed PMF\n",
    "true_pmf = model_discrete(true_theta_discrete, observed_jumps)\n",
    "\n",
    "# Add noise\n",
    "np.random.seed(43)\n",
    "noise = np.random.normal(0, 0.03 * float(jnp.max(true_pmf)), size=true_pmf.shape)\n",
    "observed_pmf = jnp.maximum(true_pmf + noise, 1e-10)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(observed_jumps, true_pmf, 'b-', linewidth=2, label='True PMF', alpha=0.7)\n",
    "plt.scatter(observed_jumps, observed_pmf, color='red', s=50, \n",
    "           label='Observed (noisy)', alpha=0.6, zorder=5)\n",
    "plt.xlabel('Number of Jumps', fontsize=12)\n",
    "plt.ylabel('PMF', fontsize=12)\n",
    "plt.title(f'Discrete Model Data (True θ = {true_theta_discrete[0]:.2f})', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Generated {len(observed_pmf)} discrete observations\")\n",
    "print(f\"Jump range: [{observed_jumps[0]}, {observed_jumps[-1]}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Run SVGD on Discrete Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVGD with discrete model\n",
    "svgd_discrete = SVGD(\n",
    "    model=model_discrete,\n",
    "    observed_data=observed_pmf,\n",
    "    theta_dim=1,\n",
    "    n_particles=30,\n",
    "    n_iterations=500,\n",
    "    learning_rate=0.01,\n",
    "    seed=44,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "svgd_discrete.fit(return_history=True)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SVGD Results (Discrete Model)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"True θ:            {true_theta_discrete[0]:.4f}\")\n",
    "print(f\"Posterior mean:    {svgd_discrete.theta_mean[0]:.4f}\")\n",
    "print(f\"Posterior std:     {svgd_discrete.theta_std[0]:.4f}\")\n",
    "print(f\"Error:             {abs(svgd_discrete.theta_mean[0] - true_theta_discrete[0]):.4f}\")\n",
    "\n",
    "# Visualize discrete results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(svgd_discrete.particles[:, 0], bins=20, density=True, alpha=0.7,\n",
    "        color='purple', edgecolor='black')\n",
    "plt.axvline(true_theta_discrete[0], color='red', linestyle='--', linewidth=2,\n",
    "           label=f'True θ = {true_theta_discrete[0]:.3f}')\n",
    "plt.axvline(svgd_discrete.theta_mean[0], color='green', linestyle='-', linewidth=2,\n",
    "           label=f'Posterior mean = {svgd_discrete.theta_mean[0]:.3f}')\n",
    "plt.xlabel('θ', fontsize=12)\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "plt.title('Posterior Distribution (Discrete Model)', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Multi-CPU Parallelization\n",
    "\n",
    "Automatic detection and usage of multiple CPUs on a single machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Automatic Resource Detection (Phase 1)\n",
    "\n",
    "Use the automatic parallelization feature from Phases 1-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize automatic parallel computing\n",
    "# This detects available CPUs and configures JAX\n",
    "config = pta.init_parallel()\n",
    "\n",
    "print(f\"Parallel Configuration:\")\n",
    "print(f\"  Environment: {config.env_info.env_type if config.env_info else 'unknown'}\")\n",
    "print(f\"  CPUs available: {config.env_info.available_cpus if config.env_info else 'N/A'}\")\n",
    "print(f\"  JAX devices: {config.device_count}\")\n",
    "print(f\"  Strategy: {config.strategy}\")\n",
    "print(f\"\\nJAX will automatically parallelize across {config.device_count} device(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 SVGD with Parallelization\n",
    "\n",
    "JAX automatically parallelizes gradient computations and particle updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Larger problem to benefit from parallelization\n",
    "n_particles_parallel = config.device_count * 10  # 10 particles per device\n",
    "\n",
    "print(f\"Running SVGD with {n_particles_parallel} particles...\")\n",
    "print(f\"Particles will be distributed across {config.device_count} device(s)\")\n",
    "print()\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "svgd_parallel = SVGD(\n",
    "    model=model_continuous,\n",
    "    observed_data=observed_pdf,\n",
    "    theta_dim=1,\n",
    "    n_particles=n_particles_parallel,\n",
    "    n_iterations=500,\n",
    "    learning_rate=0.01,\n",
    "    seed=45,\n",
    "    verbose=False  # Less output for timing\n",
    ")\n",
    "\n",
    "svgd_parallel.fit()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Multi-CPU SVGD Results\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Devices used:      {config.device_count}\")\n",
    "print(f\"Particles:         {n_particles_parallel}\")\n",
    "print(f\"Time elapsed:      {elapsed:.2f}s\")\n",
    "print(f\"Posterior mean:    {svgd_parallel.theta_mean[0]:.4f}\")\n",
    "print(f\"Posterior std:     {svgd_parallel.theta_std[0]:.4f}\")\n",
    "print(f\"Error:             {abs(svgd_parallel.theta_mean[0] - true_theta[0]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Explicit Device Specification\n",
    "\n",
    "You can also manually specify the number of CPUs to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force specific number of CPUs (if you want to limit resource usage)\n",
    "config_limited = pta.init_parallel(cpus=2)\n",
    "\n",
    "print(f\"Limited configuration:\")\n",
    "print(f\"  Devices: {config_limited.device_count}\")\n",
    "print(f\"  Strategy: {config_limited.strategy}\")\n",
    "\n",
    "# Context manager to temporarily change parallelization\n",
    "with pta.parallel_config(strategy='vmap'):\n",
    "    print(f\"\\nInside context: strategy = {pta.get_parallel_config().strategy}\")\n",
    "    # Run computation here...\n",
    "\n",
    "print(f\"Outside context: strategy = {pta.get_parallel_config().strategy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Distributed Computing (SLURM)\n",
    "\n",
    "Examples for running on SLURM clusters with multiple nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 SLURM Setup (Code for Cluster)\n",
    "\n",
    "This code detects SLURM environment and initializes distributed JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell demonstrates SLURM setup (won't actually run on local machine)\n",
    "\n",
    "# On a SLURM cluster, init_parallel() automatically detects:\n",
    "# - SLURM_JOB_ID (running in SLURM)\n",
    "# - SLURM_CPUS_PER_TASK (CPUs per node)\n",
    "# - SLURM_NTASKS (number of processes/nodes)\n",
    "# - SLURM_PROCID (process rank)\n",
    "\n",
    "# Example code that runs on SLURM:\n",
    "if False:  # Set to True when running on SLURM\n",
    "    # Automatic SLURM detection\n",
    "    config_slurm = pta.init_parallel()\n",
    "    \n",
    "    print(f\"SLURM Configuration:\")\n",
    "    print(f\"  Job ID: {config_slurm.env_info.slurm_info['job_id']}\")\n",
    "    print(f\"  Processes: {config_slurm.env_info.slurm_info['num_processes']}\")\n",
    "    print(f\"  CPUs per task: {config_slurm.env_info.slurm_info['cpus_per_task']}\")\n",
    "    print(f\"  Total devices: {config_slurm.device_count}\")\n",
    "    \n",
    "    # SVGD will automatically distribute across all nodes\n",
    "    svgd_slurm = SVGD(\n",
    "        model=model_continuous,\n",
    "        observed_data=observed_pdf,\n",
    "        theta_dim=1,\n",
    "        n_particles=config_slurm.device_count * 8,  # 8 per device\n",
    "        n_iterations=1000,\n",
    "        learning_rate=0.01,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    svgd_slurm.fit()\n",
    "\n",
    "print(\"Note: SLURM code is provided for reference.\")\n",
    "print(\"To run on SLURM, convert this notebook to a Python script:\")\n",
    "print(\"  jupyter nbconvert --to python svgd.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 SLURM Batch Script Example\n",
    "\n",
    "Save this as `submit_svgd.sh`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm_script = '''\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=svgd_inference\n",
    "#SBATCH --nodes=4\n",
    "#SBATCH --ntasks=4\n",
    "#SBATCH --cpus-per-task=8\n",
    "#SBATCH --time=02:00:00\n",
    "#SBATCH --output=logs/svgd_%j.out\n",
    "#SBATCH --error=logs/svgd_%j.err\n",
    "\n",
    "# Load modules\n",
    "module load python/3.10\n",
    "\n",
    "# Activate environment\n",
    "source venv/bin/activate\n",
    "\n",
    "# Run with srun for multi-node\n",
    "srun python svgd_script.py\n",
    "'''\n",
    "\n",
    "print(\"SLURM batch script:\")\n",
    "print(slurm_script)\n",
    "print(\"\\nSubmit with: sbatch submit_svgd.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Advanced Options\n",
    "\n",
    "Explore additional SVGD features and options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Custom Prior Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom log prior\n",
    "def log_prior_uniform(theta, lower=0.1, upper=2.0):\n",
    "    \"\"\"Uniform prior on [lower, upper]\"\"\"\n",
    "    if jnp.all((theta >= lower) & (theta <= upper)):\n",
    "        return 0.0  # Uniform within bounds\n",
    "    else:\n",
    "        return -jnp.inf  # Zero probability outside bounds\n",
    "\n",
    "def log_prior_gamma(theta, alpha=2.0, beta=2.0):\n",
    "    \"\"\"Gamma prior: Gamma(alpha, beta)\"\"\"\n",
    "    # log p(theta) = (alpha-1)*log(theta) - beta*theta - log(Gamma(alpha)) + alpha*log(beta)\n",
    "    return (alpha - 1) * jnp.log(theta) - beta * theta\n",
    "\n",
    "# SVGD with custom prior\n",
    "svgd_custom_prior = SVGD(\n",
    "    model=model_continuous,\n",
    "    observed_data=observed_pdf,\n",
    "    prior=log_prior_gamma,  # Use Gamma prior\n",
    "    theta_dim=1,\n",
    "    n_particles=30,\n",
    "    n_iterations=500,\n",
    "    learning_rate=0.01,\n",
    "    seed=46,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "svgd_custom_prior.fit()\n",
    "\n",
    "print(f\"SVGD with Custom Prior (Gamma):\")\n",
    "print(f\"  Posterior mean: {svgd_custom_prior.theta_mean[0]:.4f}\")\n",
    "print(f\"  Posterior std:  {svgd_custom_prior.theta_std[0]:.4f}\")\n",
    "\n",
    "# Compare with default prior\n",
    "print(f\"\\nComparison with Default Prior (Normal):\")\n",
    "print(f\"  Gamma prior mean:  {svgd_custom_prior.theta_mean[0]:.4f}\")\n",
    "print(f\"  Normal prior mean: {svgd.theta_mean[0]:.4f}\")\n",
    "print(f\"  Difference:        {abs(svgd_custom_prior.theta_mean[0] - svgd.theta_mean[0]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Different Kernel Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different kernel bandwidths\n",
    "kernel_options = ['rbf_median', 'rbf_adaptive', 1.0]  # 1.0 = fixed bandwidth\n",
    "\n",
    "kernel_results = {}\n",
    "for kernel_opt in kernel_options:\n",
    "    svgd_kernel = SVGD(\n",
    "        model=model_continuous,\n",
    "        observed_data=observed_pdf,\n",
    "        theta_dim=1,\n",
    "        n_particles=30,\n",
    "        n_iterations=500,\n",
    "        learning_rate=0.01,\n",
    "        kernel=kernel_opt,\n",
    "        seed=47,\n",
    "        verbose=False\n",
    "    )\n",
    "    svgd_kernel.fit()\n",
    "    kernel_results[str(kernel_opt)] = svgd_kernel\n",
    "\n",
    "# Compare results\n",
    "print(f\"Kernel Comparison:\")\n",
    "print(f\"{'Kernel':<20} {'Posterior Mean':<20} {'Posterior Std':<20}\")\n",
    "print(\"-\" * 60)\n",
    "for kernel_opt, result in kernel_results.items():\n",
    "    print(f\"{kernel_opt:<20} {result.theta_mean[0]:<20.4f} {result.theta_std[0]:<20.4f}\")\n",
    "\n",
    "print(f\"\\nNote: 'rbf_median' (median heuristic) usually works best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different learning rates\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "\n",
    "lr_results = []\n",
    "for lr in learning_rates:\n",
    "    svgd_lr = SVGD(\n",
    "        model=model_continuous,\n",
    "        observed_data=observed_pdf,\n",
    "        theta_dim=1,\n",
    "        n_particles=30,\n",
    "        n_iterations=500,\n",
    "        learning_rate=lr,\n",
    "        seed=48,\n",
    "        verbose=False\n",
    "    )\n",
    "    svgd_lr.fit()\n",
    "    error = abs(svgd_lr.theta_mean[0] - true_theta[0])\n",
    "    lr_results.append((lr, svgd_lr.theta_mean[0], error))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "lrs, means, errors = zip(*lr_results)\n",
    "ax.plot(lrs, errors, 'o-', linewidth=2, markersize=10)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Learning Rate', fontsize=12)\n",
    "ax.set_ylabel('Estimation Error', fontsize=12)\n",
    "ax.set_title('Effect of Learning Rate on SVGD', fontsize=14)\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_lr, best_mean, best_error = min(lr_results, key=lambda x: x[2])\n",
    "print(f\"Best learning rate: {best_lr}\")\n",
    "print(f\"Best error: {best_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Moment-Based Regularization\n",
    "\n",
    "Use moment regularization to improve stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model that returns both PMF and moments\n",
    "model_with_moments = pta.Graph.pmf_and_moments_from_graph(graph, nr_moments=2)\n",
    "\n",
    "# Generate observation times (actual waiting times)\n",
    "np.random.seed(49)\n",
    "n_obs = 20\n",
    "observation_times = np.random.exponential(1.0 / true_theta[0], n_obs)\n",
    "observation_times = jnp.array(observation_times)\n",
    "\n",
    "# Compute observed PMF at evaluation points\n",
    "eval_times = jnp.linspace(0.1, 5.0, 30)\n",
    "observed_pmf_moments, _ = model_with_moments(true_theta, eval_times)\n",
    "\n",
    "# Add noise\n",
    "noise = np.random.normal(0, 0.05 * float(jnp.max(observed_pmf_moments)), \n",
    "                         size=observed_pmf_moments.shape)\n",
    "observed_pmf_moments = jnp.maximum(observed_pmf_moments + noise, 1e-10)\n",
    "\n",
    "# SVGD with moment regularization\n",
    "svgd_regularized = SVGD(\n",
    "    model=model_with_moments,\n",
    "    observed_data=observed_pmf_moments,\n",
    "    theta_dim=1,\n",
    "    n_particles=30,\n",
    "    n_iterations=500,\n",
    "    learning_rate=0.01,\n",
    "    seed=50,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Use fit_regularized instead of fit\n",
    "svgd_regularized.fit_regularized(\n",
    "    observed_times=observation_times,\n",
    "    nr_moments=2,\n",
    "    regularization=1.0,  # Regularization strength\n",
    "    return_history=True\n",
    ")\n",
    "\n",
    "print(f\"SVGD with Moment Regularization:\")\n",
    "print(f\"  Posterior mean:    {svgd_regularized.theta_mean[0]:.4f}\")\n",
    "print(f\"  Posterior std:     {svgd_regularized.theta_std[0]:.4f}\")\n",
    "print(f\"  Sample moments:    {svgd_regularized.sample_moments}\")\n",
    "print(f\"\\nRegularization adds moment-matching penalty to improve stability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Batching and Sharding\n",
    "\n",
    "Understanding how data is distributed across devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Understanding Particle Distribution\n",
    "\n",
    "SVGD particles are automatically distributed across devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current configuration\n",
    "current_config = pta.get_parallel_config()\n",
    "\n",
    "if current_config:\n",
    "    n_devices = current_config.device_count\n",
    "    n_particles = 40\n",
    "    \n",
    "    print(f\"Particle Distribution:\")\n",
    "    print(f\"  Total devices:       {n_devices}\")\n",
    "    print(f\"  Total particles:     {n_particles}\")\n",
    "    print(f\"  Particles per device: {n_particles // n_devices}\")\n",
    "    print(f\"\\nJAX automatically handles:\")\n",
    "    print(f\"  1. Sharding particles across devices\")\n",
    "    print(f\"  2. Parallel gradient computation\")\n",
    "    print(f\"  3. Communication between devices\")\n",
    "    print(f\"  4. Result gathering\")\n",
    "else:\n",
    "    print(\"No parallel configuration set. Using default (single device).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Manual Device Placement (Advanced)\n",
    "\n",
    "For fine-grained control over device placement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get available devices\n",
    "devices = jax.devices()\n",
    "print(f\"Available JAX devices:\")\n",
    "for i, device in enumerate(devices):\n",
    "    print(f\"  Device {i}: {device}\")\n",
    "\n",
    "# Example: Manually place data on specific device\n",
    "if len(devices) > 1:\n",
    "    # Put initial particles on device 0\n",
    "    theta_init_device = jax.device_put(theta_init, devices[0])\n",
    "    print(f\"\\nPlaced initial particles on: {devices[0]}\")\n",
    "else:\n",
    "    print(f\"\\nSingle device detected - manual placement not needed\")\n",
    "\n",
    "# Note: SVGD class handles device placement automatically\n",
    "# Manual placement is rarely needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Batch Size Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Choosing Number of Particles:\")\n",
    "print(f\"\\nGuidelines:\")\n",
    "print(f\"  • Start with 30-50 particles for simple problems\")\n",
    "print(f\"  • Use 100+ particles for complex posteriors\")\n",
    "print(f\"  • Scale particles with number of devices (e.g., 10 per device)\")\n",
    "print(f\"  • More particles = better posterior approximation but slower\")\n",
    "print(f\"\\nMemory considerations:\")\n",
    "print(f\"  • Each particle requires model evaluation at all data points\")\n",
    "print(f\"  • Memory usage ≈ n_particles × n_data_points × model_complexity\")\n",
    "print(f\"  • If OOM error, reduce n_particles or n_data_points\")\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  • Computation scales roughly linearly with n_particles\")\n",
    "print(f\"  • Parallelization provides speedup up to n_devices\")\n",
    "print(f\"  • Communication overhead increases with more devices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7: Troubleshooting and Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Common Issues and Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "troubleshooting_guide = \"\"\"\n",
    "COMMON ISSUES AND SOLUTIONS:\n",
    "\n",
    "1. NaN or Inf in gradients\n",
    "   Problem: Model returns very small/large values causing numerical instability\n",
    "   Solution: \n",
    "     • Add small epsilon to prevent log(0): jnp.log(model_output + 1e-10)\n",
    "     • Reduce learning rate\n",
    "     • Check model for numerical issues\n",
    "\n",
    "2. Poor convergence\n",
    "   Problem: Particles don't converge to true posterior\n",
    "   Solution:\n",
    "     • Increase n_iterations (try 1000-2000)\n",
    "     • Adjust learning rate (try 0.001, 0.01, 0.1)\n",
    "     • Increase n_particles (try 50-100)\n",
    "     • Check if observed data is informative\n",
    "\n",
    "3. Out of memory\n",
    "   Problem: Too many particles or data points\n",
    "   Solution:\n",
    "     • Reduce n_particles\n",
    "     • Subsample observed data\n",
    "     • Use batch processing for large datasets\n",
    "\n",
    "4. Slow performance\n",
    "   Problem: Computation takes too long\n",
    "   Solution:\n",
    "     • Initialize parallel computing: pta.init_parallel()\n",
    "     • Reduce n_particles or n_iterations\n",
    "     • Use JIT compilation (automatic in model)\n",
    "     • Run on SLURM cluster with multiple nodes\n",
    "\n",
    "5. JAX device configuration issues\n",
    "   Problem: JAX not using multiple CPUs\n",
    "   Solution:\n",
    "     • Call pta.init_parallel() BEFORE importing JAX\n",
    "     • Set XLA_FLAGS before importing (done automatically by init_parallel)\n",
    "     • Verify: jax.devices() shows multiple devices\n",
    "\n",
    "6. Model signature errors\n",
    "   Problem: \"Model must have signature model(theta, times)\"\n",
    "   Solution:\n",
    "     • Ensure model = Graph.pmf_from_graph(graph) or similar\n",
    "     • Model should take (theta, times) as arguments\n",
    "     • Test: model(jnp.array([1.0]), jnp.array([0.5, 1.0]))\n",
    "\n",
    "7. Discrete vs Continuous confusion\n",
    "   Problem: Using wrong model type\n",
    "   Solution:\n",
    "     • Continuous: Graph.pmf_from_graph(graph, discrete=False)\n",
    "       - Times are float values\n",
    "       - Evaluates PDF\n",
    "     • Discrete: Graph.pmf_from_graph(graph, discrete=True)\n",
    "       - Times are integer jump counts\n",
    "       - Evaluates PMF\n",
    "\n",
    "8. Prior specification\n",
    "   Problem: Unclear how to specify custom prior\n",
    "   Solution:\n",
    "     • Define log_prior(theta) -> scalar\n",
    "     • Return log probability (not probability)\n",
    "     • Default is standard normal: -0.5 * sum(theta^2)\n",
    "     • Example: lambda theta: -0.5 * jnp.sum((theta - mu)**2 / sigma**2)\n",
    "\"\"\"\n",
    "\n",
    "print(troubleshooting_guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Best Practices Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_practices = \"\"\"\n",
    "SVGD BEST PRACTICES:\n",
    "\n",
    "✓ Initialization\n",
    "  • Call pta.init_parallel() at notebook/script start\n",
    "  • Initialize particles from reasonable values (not too far from posterior)\n",
    "  • Use random seed for reproducibility\n",
    "\n",
    "✓ Model Building\n",
    "  • Test model before SVGD: model(test_theta, test_times)\n",
    "  • Ensure model returns positive values for likelihood\n",
    "  • Add epsilon to prevent numerical issues: model_output + 1e-10\n",
    "  • Normalize graph if needed: graph.normalize()\n",
    "\n",
    "✓ Data Preparation\n",
    "  • Use sufficient data points (at least 10-20)\n",
    "  • Ensure data is informative about parameters\n",
    "  • Consider subsampling very large datasets\n",
    "  • Check for outliers or data quality issues\n",
    "\n",
    "✓ SVGD Configuration\n",
    "  • Start with n_particles=30-50, increase if needed\n",
    "  • Start with learning_rate=0.01, adjust if not converging\n",
    "  • Use n_iterations=500-1000 for initial runs\n",
    "  • Use 'rbf_median' kernel (default)\n",
    "\n",
    "✓ Convergence Monitoring\n",
    "  • Use return_history=True to track convergence\n",
    "  • Plot particle trajectories\n",
    "  • Check if posterior mean stabilizes\n",
    "  • Compare posterior predictive to observed data\n",
    "\n",
    "✓ Validation\n",
    "  • Test on synthetic data with known parameters\n",
    "  • Check if true parameter is in credible interval\n",
    "  • Compare posterior predictive to observations\n",
    "  • Try different random seeds to check stability\n",
    "\n",
    "✓ Performance\n",
    "  • Use pta.init_parallel() for multi-CPU\n",
    "  • Scale particles with devices (10 per device)\n",
    "  • Use SLURM for very large problems\n",
    "  • Profile code to identify bottlenecks\n",
    "\n",
    "✓ Documentation\n",
    "  • Document model parameterization\n",
    "  • Record hyperparameters used\n",
    "  • Save results for reproducibility\n",
    "  • Note any convergence issues\n",
    "\"\"\"\n",
    "\n",
    "print(best_practices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Diagnostic Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_svgd_convergence(svgd_obj, true_param=None):\n",
    "    \"\"\"\n",
    "    Perform diagnostic checks on SVGD results.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    svgd_obj : SVGD\n",
    "        Fitted SVGD object\n",
    "    true_param : array, optional\n",
    "        True parameter value (if known)\n",
    "    \"\"\"\n",
    "    print(\"SVGD Convergence Diagnostics\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check 1: Particle diversity\n",
    "    particle_std = jnp.std(svgd_obj.particles, axis=0)\n",
    "    print(f\"\\n1. Particle Diversity:\")\n",
    "    print(f\"   Posterior std: {svgd_obj.theta_std}\")\n",
    "    if jnp.any(particle_std < 0.01):\n",
    "        print(f\"   ⚠ Warning: Very low diversity. Particles may have collapsed.\")\n",
    "        print(f\"      Try: Lower learning rate or increase n_particles\")\n",
    "    elif jnp.any(particle_std > 2.0):\n",
    "        print(f\"   ⚠ Warning: Very high diversity. Particles may not have converged.\")\n",
    "        print(f\"      Try: Increase n_iterations or adjust learning_rate\")\n",
    "    else:\n",
    "        print(f\"   ✓ Particle diversity looks reasonable\")\n",
    "    \n",
    "    # Check 2: Convergence history\n",
    "    if svgd_obj.history is not None:\n",
    "        history = jnp.stack(svgd_obj.history)\n",
    "        final_mean = jnp.mean(history[-1, :, 0])\n",
    "        mid_mean = jnp.mean(history[len(history)//2, :, 0])\n",
    "        change = abs(final_mean - mid_mean)\n",
    "        \n",
    "        print(f\"\\n2. Convergence:\")\n",
    "        print(f\"   Mean at iteration {len(history)//2}: {mid_mean:.4f}\")\n",
    "        print(f\"   Mean at final iteration: {final_mean:.4f}\")\n",
    "        print(f\"   Change in second half: {change:.4f}\")\n",
    "        \n",
    "        if change > 0.1:\n",
    "            print(f\"   ⚠ Warning: Mean still changing significantly\")\n",
    "            print(f\"      Try: Increase n_iterations\")\n",
    "        else:\n",
    "            print(f\"   ✓ Convergence looks good\")\n",
    "    \n",
    "    # Check 3: True parameter coverage (if known)\n",
    "    if true_param is not None:\n",
    "        true_val = true_param[0] if hasattr(true_param, '__len__') else true_param\n",
    "        lower = svgd_obj.theta_mean[0] - 1.96 * svgd_obj.theta_std[0]\n",
    "        upper = svgd_obj.theta_mean[0] + 1.96 * svgd_obj.theta_std[0]\n",
    "        \n",
    "        print(f\"\\n3. Parameter Recovery:\")\n",
    "        print(f\"   True parameter: {true_val:.4f}\")\n",
    "        print(f\"   Posterior mean: {svgd_obj.theta_mean[0]:.4f}\")\n",
    "        print(f\"   Error: {abs(svgd_obj.theta_mean[0] - true_val):.4f}\")\n",
    "        print(f\"   95% CI: [{lower:.4f}, {upper:.4f}]\")\n",
    "        \n",
    "        if lower <= true_val <= upper:\n",
    "            print(f\"   ✓ True parameter within 95% credible interval\")\n",
    "        else:\n",
    "            print(f\"   ⚠ Warning: True parameter outside 95% CI\")\n",
    "            print(f\"      Possible issues: Not enough data, wrong model, or need more iterations\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "\n",
    "# Run diagnostic on our SVGD result\n",
    "check_svgd_convergence(svgd, true_param=true_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "## What We Covered\n",
    "\n",
    "1. **Basic SVGD** - Continuous coalescent model inference\n",
    "2. **Discrete Models** - SVGD with discrete phase-type distributions\n",
    "3. **Multi-CPU** - Automatic parallelization with `init_parallel()`\n",
    "4. **Distributed** - SLURM cluster setup and execution\n",
    "5. **Advanced** - Custom priors, kernels, hyperparameter tuning, regularization\n",
    "6. **Batching/Sharding** - Understanding particle distribution across devices\n",
    "7. **Troubleshooting** - Common issues, best practices, diagnostics\n",
    "\n",
    "## Quick Reference\n",
    "\n",
    "```python\n",
    "# Basic workflow\n",
    "import ptdalgorithms as pta\n",
    "\n",
    "# 1. Initialize parallelization\n",
    "pta.init_parallel()\n",
    "\n",
    "# 2. Build parameterized model\n",
    "graph = pta.Graph(callback=my_callback, parameterized=True)\n",
    "model = pta.Graph.pmf_from_graph(graph, discrete=False)\n",
    "\n",
    "# 3. Run SVGD\n",
    "results = pta.Graph.svgd(\n",
    "    model=model,\n",
    "    observed_data=data,\n",
    "    theta_dim=1,\n",
    "    n_particles=50,\n",
    "    n_iterations=1000,\n",
    "    learning_rate=0.01\n",
    ")\n",
    "\n",
    "# 4. Analyze\n",
    "print(results['theta_mean'])\n",
    "print(results['theta_std'])\n",
    "```\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [PtDAlgorithms Documentation](https://docs.ptdalgorithms.com)\n",
    "- [SVGD Paper](https://arxiv.org/abs/1608.04471)\n",
    "- [JAX Documentation](https://jax.readthedocs.io)\n",
    "- Example notebooks in `docs/examples/python/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
